{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "901-keras_regression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/parekhakhil/pyImageSearch/blob/main/901_keras_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4RpK9pawQzP"
      },
      "source": [
        "# Regression with Keras\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ntZ1AkXZIxY"
      },
      "source": [
        "## Welcome to **[PyImageSearch Plus](http://pyimg.co/plus)** Jupyter Notebooks!\n",
        "\n",
        "This notebook is associated with the [Regression with Keras](https://www.pyimagesearch.com/2019/01/21/regression-with-keras/) blog post published on 2019-01-21.\n",
        "\n",
        "Only the code for the blog post is here. Most codeblocks have a 1:1 relationship with what you find in the blog post with two exceptions: (1) Python classes are not separate files as they are typically organized with PyImageSearch projects, and (2) Command Line Argument parsing is replaced with an `args` dictionary that you can manipulate as needed.\n",
        "\n",
        "We recommend that you execute (press ▶️) the code block-by-block, as-is, before adjusting parameters and `args` inputs. Once you've verified that the code is working, you are welcome to hack with it and learn from manipulating inputs, settings, and parameters. For more information on using Jupyter and Colab, please refer to these resources:\n",
        "\n",
        "*   [Jupyter Notebook User Interface](https://jupyter-notebook.readthedocs.io/en/stable/notebook.html#notebook-user-interface)\n",
        "*   [Overview of Google Colaboratory Features](https://colab.research.google.com/notebooks/basic_features_overview.ipynb)\n",
        "\n",
        "\n",
        "Happy hacking!\n",
        "\n",
        "<hr>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFhAzQB3aNMa"
      },
      "source": [
        "### Download the code zip file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7y0LG1EuaRlB"
      },
      "source": [
        "!wget https://s3-us-west-2.amazonaws.com/static.pyimagesearch.com/keras-regression/keras-regression.zip\n",
        "!unzip -qq keras-regression.zip\n",
        "%cd keras-regression"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtB164Pkw8Rj"
      },
      "source": [
        "### Downloading the House Prices Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sl-qEDgzw9kq"
      },
      "source": [
        "!git clone https://github.com/emanhamed/Houses-dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SgTVT3HagGZ"
      },
      "source": [
        "## Blog Post Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcrOk6pURp50"
      },
      "source": [
        "### Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJaCNlDDRz6d"
      },
      "source": [
        "# import the necessary packages\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import argparse\n",
        "import locale\n",
        "import glob\n",
        "import cv2\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBrLwCtN5kqy"
      },
      "source": [
        "### Loading the House Prices Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekX9WjXswgyB"
      },
      "source": [
        "def load_house_attributes(inputPath):\n",
        "\t# initialize the list of column names in the CSV file and then\n",
        "\t# load it using Pandas\n",
        "\tcols = [\"bedrooms\", \"bathrooms\", \"area\", \"zipcode\", \"price\"]\n",
        "\tdf = pd.read_csv(inputPath, sep=\" \", header=None, names=cols)\n",
        "\n",
        "\t# determine (1) the unique zip codes and (2) the number of data\n",
        "\t# points with each zip code\n",
        "\tzipcodes = df[\"zipcode\"].value_counts().keys().tolist()\n",
        "\tcounts = df[\"zipcode\"].value_counts().tolist()\n",
        "\n",
        "\t# loop over each of the unique zip codes and their corresponding\n",
        "\t# count\n",
        "\tfor (zipcode, count) in zip(zipcodes, counts):\n",
        "\t\t# the zip code counts for our housing dataset is *extremely*\n",
        "\t\t# unbalanced (some only having 1 or 2 houses per zip code)\n",
        "\t\t# so let's sanitize our data by removing any houses with less\n",
        "\t\t# than 25 houses per zip code\n",
        "\t\tif count < 25:\n",
        "\t\t\tidxs = df[df[\"zipcode\"] == zipcode].index\n",
        "\t\t\tdf.drop(idxs, inplace=True)\n",
        "\n",
        "\t# return the data frame\n",
        "\treturn df\n",
        "\n",
        "def process_house_attributes(df, train, test):\n",
        "\t# initialize the column names of the continuous data\n",
        "\tcontinuous = [\"bedrooms\", \"bathrooms\", \"area\"]\n",
        "\n",
        "\t# performin min-max scaling each continuous feature column to\n",
        "\t# the range [0, 1]\n",
        "\tcs = MinMaxScaler()\n",
        "\ttrainContinuous = cs.fit_transform(train[continuous])\n",
        "\ttestContinuous = cs.transform(test[continuous])\n",
        "\n",
        "\t# one-hot encode the zip code categorical data (by definition of\n",
        "\t# one-hot encoing, all output features are now in the range [0, 1])\n",
        "\tzipBinarizer = LabelBinarizer().fit(df[\"zipcode\"])\n",
        "\ttrainCategorical = zipBinarizer.transform(train[\"zipcode\"])\n",
        "\ttestCategorical = zipBinarizer.transform(test[\"zipcode\"])\n",
        "\n",
        "\t# construct our training and testing data points by concatenating\n",
        "\t# the categorical features with the continuous features\n",
        "\ttrainX = np.hstack([trainCategorical, trainContinuous])\n",
        "\ttestX = np.hstack([testCategorical, testContinuous])\n",
        "\n",
        "\t# return the concatenated training and testing data\n",
        "\treturn (trainX, testX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZH3P0HmxsN8"
      },
      "source": [
        "### Implementing a Neural Network for Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gt28D10Ixcgi"
      },
      "source": [
        "def create_mlp(dim, regress=False):\n",
        "\t# define our MLP network\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(8, input_dim=dim, activation=\"relu\"))\n",
        "\tmodel.add(Dense(4, activation=\"relu\"))\n",
        "\n",
        "\t# check to see if the regression node should be added\n",
        "\tif regress:\n",
        "\t\tmodel.add(Dense(1, activation=\"linear\"))\n",
        "\n",
        "\t# return our model\n",
        "\treturn model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KDe7-pNx78N"
      },
      "source": [
        "### Implementing our Keras Regression Script"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsQ3ObChx5BG"
      },
      "source": [
        "# # construct the argument parser and parse the arguments\n",
        "# ap = argparse.ArgumentParser()\n",
        "# ap.add_argument(\"-d\", \"--dataset\", type=str, required=True,\n",
        "# \thelp=\"path to input dataset of house images\")\n",
        "# args = vars(ap.parse_args())\n",
        "\n",
        "# since we are using Jupyter Notebooks we can replace our argument\n",
        "# parsing code with *hard coded* arguments and values\n",
        "args = {\n",
        "\t\"dataset\": \"Houses-dataset/Houses Dataset\",\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpHpOMLlynE_"
      },
      "source": [
        "# construct the path to the input .txt file that contains information\n",
        "# on each house in the dataset and then load the dataset\n",
        "print(\"[INFO] loading house attributes...\")\n",
        "inputPath = os.path.sep.join([args[\"dataset\"], \"HousesInfo.txt\"])\n",
        "df = load_house_attributes(inputPath)\n",
        "\n",
        "# construct a training and testing split with 75% of the data used\n",
        "# for training and the remaining 25% for evaluation\n",
        "print(\"[INFO] constructing training/testing split...\")\n",
        "(train, test) = train_test_split(df, test_size=0.25, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbR9ruTkyqit"
      },
      "source": [
        "# find the largest house price in the training set and use it to\n",
        "# scale our house prices to the range [0, 1] (this will lead to\n",
        "# better training and convergence)\n",
        "maxPrice = train[\"price\"].max()\n",
        "trainY = train[\"price\"] / maxPrice\n",
        "testY = test[\"price\"] / maxPrice"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKZraZ_CzUK2"
      },
      "source": [
        "# process the house attributes data by performing min-max scaling\n",
        "# on continuous features, one-hot encoding on categorical features,\n",
        "# and then finally concatenating them together\n",
        "print(\"[INFO] processing data...\")\n",
        "(trainX, testX) = process_house_attributes(df, train, test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtmT2KcqzW8K"
      },
      "source": [
        "# create our MLP and then compile the model using mean absolute\n",
        "# percentage error as our loss, implying that we seek to minimize\n",
        "# the absolute percentage difference between our price *predictions*\n",
        "# and the *actual prices*\n",
        "model = create_mlp(trainX.shape[1], regress=True)\n",
        "opt = Adam(lr=1e-3, decay=1e-3 / 200)\n",
        "model.compile(loss=\"mean_absolute_percentage_error\", optimizer=opt)\n",
        "\n",
        "# train the model\n",
        "print(\"[INFO] training model...\")\n",
        "model.fit(x=trainX, y=trainY, \n",
        "\tvalidation_data=(testX, testY),\n",
        "\tepochs=200, batch_size=8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KO5UiyuozdaT"
      },
      "source": [
        "# make predictions on the testing data\n",
        "print(\"[INFO] predicting house prices...\")\n",
        "preds = model.predict(testX)\n",
        "\n",
        "# compute the difference between the *predicted* house prices and the\n",
        "# *actual* house prices, then compute the percentage difference and\n",
        "# the absolute percentage difference\n",
        "diff = preds.flatten() - testY\n",
        "percentDiff = (diff / testY) * 100\n",
        "absPercentDiff = np.abs(percentDiff)\n",
        "\n",
        "# compute the mean and standard deviation of the absolute percentage\n",
        "# difference\n",
        "mean = np.mean(absPercentDiff)\n",
        "std = np.std(absPercentDiff)\n",
        "\n",
        "# finally, show some statistics on our model\n",
        "locale.setlocale(locale.LC_ALL, \"en_US.UTF-8\")\n",
        "print(\"[INFO] avg. house price: {}, std house price: {}\".format(\n",
        "\tlocale.currency(df[\"price\"].mean(), grouping=True),\n",
        "\tlocale.currency(df[\"price\"].std(), grouping=True)))\n",
        "print(\"[INFO] mean: {:.2f}%, std: {:.2f}%\".format(mean, std))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ogkNauArL6u"
      },
      "source": [
        "For a detailed walkthrough of the concepts and code, be sure to refer to the full tutorial, [*Regression with Keras*](https://www.pyimagesearch.com/2019/01/21/regression-with-keras/) blog post published on 2019-01-21."
      ]
    }
  ]
}