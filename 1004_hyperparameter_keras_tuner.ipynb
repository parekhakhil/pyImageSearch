{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1004-hyperparameter_keras_tuner.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/parekhakhil/pyImageSearch/blob/main/1004_hyperparameter_keras_tuner.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4RpK9pawQzP"
      },
      "source": [
        "# Easy Hyperparameter Tuning with Keras Tuner and TensorFlow\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ntZ1AkXZIxY"
      },
      "source": [
        "\n",
        "\n",
        "This notebook is associated with the [Easy Hyperparameter Tuning with Keras Tuner and TensorFlow](https://www.pyimagesearch.com/2021/06/07/easy-hyperparameter-tuning-with-keras-tuner-and-tensorflow/) blog post published on 2021-06-07.\n",
        "\n",
        "Only the code for the blog post is here. Most codeblocks have a 1:1 relationship with what you find in the blog post with two exceptions: (1) Python classes are not separate files as they are typically organized with PyImageSearch projects, and (2) Command Line Argument parsing is replaced with an `args` dictionary that you can manipulate as needed.\n",
        "\n",
        "We recommend that you execute (press ▶️) the code block-by-block, as-is, before adjusting parameters and `args` inputs. Once you've verified that the code is working, you are welcome to hack with it and learn from manipulating inputs, settings, and parameters. For more information on using Jupyter and Colab, please refer to these resources:\n",
        "\n",
        "*   [Jupyter Notebook User Interface](https://jupyter-notebook.readthedocs.io/en/stable/notebook.html#notebook-user-interface)\n",
        "*   [Overview of Google Colaboratory Features](https://colab.research.google.com/notebooks/basic_features_overview.ipynb)\n",
        "\n",
        " \n",
        "\n",
        "Happy hacking!\n",
        "\n",
        "\n",
        "\n",
        "<hr>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LY860Q9z8436"
      },
      "source": [
        "### Install the necessary packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w69SzOKK86rN"
      },
      "source": [
        "!pip install keras-tuner"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFhAzQB3aNMa"
      },
      "source": [
        "### Download the code zip file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7y0LG1EuaRlB"
      },
      "source": [
        "!wget https://pyimagesearch-code-downloads.s3-us-west-2.amazonaws.com/hyperparameter-keras-tuner/hyperparameter-keras-tuner.zip\n",
        "!unzip -qq hyperparameter-keras-tuner.zip\n",
        "%cd hyperparameter-keras-tuner"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SgTVT3HagGZ"
      },
      "source": [
        "## Blog Post Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcrOk6pURp50"
      },
      "source": [
        "### Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJaCNlDDRz6d"
      },
      "source": [
        "# set the matplotlib backend so figures can be saved in the background\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "\n",
        "# import the necessary package\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras import backend as K\n",
        "from sklearn.metrics import classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import kerastuner as kt\n",
        "import numpy as np\n",
        "import argparse\n",
        "import cv2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBrLwCtN5kqy"
      },
      "source": [
        "### Implementing our Config class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ab1zz-lQ9K3M"
      },
      "source": [
        "class Config:\n",
        "    # define the path to our output directory\n",
        "    OUTPUT_PATH = \"output\"\n",
        "\n",
        "    # initialize the input shape and number of classes\n",
        "    INPUT_SHAPE = (28, 28, 1)\n",
        "    NUM_CLASSES = 10\n",
        "\n",
        "    # define the total number of epochs to train, batch size, and the\n",
        "    # early stopping patience\n",
        "    EPOCHS = 50\n",
        "    BS = 32\n",
        "    EARLY_STOPPING_PATIENCE = 5\n",
        "\n",
        "# instantiate an object of the configuration class\n",
        "config = Config()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ql-F1dT99oqS"
      },
      "source": [
        "### Implementing our plotting helper function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdV_hZvo9p09"
      },
      "source": [
        "def save_plot(H, path):\n",
        "\t# plot the training loss and accuracy\n",
        "\tplt.style.use(\"ggplot\")\n",
        "\tplt.figure()\n",
        "\tplt.plot(H.history[\"loss\"], label=\"train_loss\")\n",
        "\tplt.plot(H.history[\"val_loss\"], label=\"val_loss\")\n",
        "\tplt.plot(H.history[\"accuracy\"], label=\"train_acc\")\n",
        "\tplt.plot(H.history[\"val_accuracy\"], label=\"val_acc\")\n",
        "\tplt.title(\"Training Loss and Accuracy\")\n",
        "\tplt.xlabel(\"Epoch #\")\n",
        "\tplt.ylabel(\"Loss/Accuracy\")\n",
        "\tplt.legend()\n",
        "\tplt.savefig(path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1Qg4cNv96o8"
      },
      "source": [
        "### Creating our CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tc9wiaGC-Cl6"
      },
      "source": [
        "def build_model(hp):\n",
        "\t# initialize the model along with the input shape and channel\n",
        "\t# dimension\n",
        "\tmodel = Sequential()\n",
        "\tinputShape = config.INPUT_SHAPE\n",
        "\tchanDim = -1\n",
        "\n",
        "\t# first CONV => RELU => POOL layer set\n",
        "\tmodel.add(Conv2D(\n",
        "\t\thp.Int(\"conv_1\", min_value=32, max_value=96, step=32),\n",
        "\t\t(3, 3), padding=\"same\", input_shape=inputShape))\n",
        "\tmodel.add(Activation(\"relu\"))\n",
        "\tmodel.add(BatchNormalization(axis=chanDim))\n",
        "\tmodel.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "\t# second CONV => RELU => POOL layer set\n",
        "\tmodel.add(Conv2D(\n",
        "\t\thp.Int(\"conv_2\", min_value=64, max_value=128, step=32),\n",
        "\t\t(3, 3), padding=\"same\"))\n",
        "\tmodel.add(Activation(\"relu\"))\n",
        "\tmodel.add(BatchNormalization(axis=chanDim))\n",
        "\tmodel.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "\t# first (and only) set of FC => RELU layers\n",
        "\tmodel.add(Flatten())\n",
        "\tmodel.add(Dense(hp.Int(\"dense_units\", min_value=256,\n",
        "\t\tmax_value=768, step=256)))\n",
        "\tmodel.add(Activation(\"relu\"))\n",
        "\tmodel.add(BatchNormalization())\n",
        "\tmodel.add(Dropout(0.5))\n",
        "\n",
        "\t# softmax classifier\n",
        "\tmodel.add(Dense(config.NUM_CLASSES))\n",
        "\tmodel.add(Activation(\"softmax\"))\n",
        "\n",
        "\t# initialize the learning rate choices and optimizer\n",
        "\tlr = hp.Choice(\"learning_rate\",\n",
        "\t\tvalues=[1e-1, 1e-2, 1e-3])\n",
        "\topt = Adam(learning_rate=lr)\n",
        "\n",
        "\t# compile the model\n",
        "\tmodel.compile(optimizer=opt, loss=\"categorical_crossentropy\",\n",
        "\t\tmetrics=[\"accuracy\"])\n",
        "\n",
        "\t# return the model\n",
        "\treturn model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jppw5-Bd56H-"
      },
      "source": [
        "### Implementing hyperparameter tuning with Keras Tuner"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okM7Bpyeq8Kc"
      },
      "source": [
        "# # construct the argument parser and parse the arguments\n",
        "# ap = argparse.ArgumentParser()\n",
        "# ap.add_argument(\"-t\", \"--tuner\", required=True, type=str,\n",
        "# \tchoices=[\"hyperband\", \"random\", \"bayesian\"],\n",
        "# \thelp=\"type of hyperparameter tuner we'll be using\")\n",
        "# ap.add_argument(\"-p\", \"--plot\", required=True,\n",
        "# \thelp=\"path to output accuracy/loss plot\")\n",
        "# args = vars(ap.parse_args())\n",
        "\n",
        "# since we are using Jupyter Notebooks we can replace our argument\n",
        "# parsing code with *hard coded* arguments and values\n",
        "args = {\n",
        "\t\"tuner\": \"random\",\n",
        "\t\"plot\": \"output/random_plot.png\"\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7o5QRa7-k0R"
      },
      "source": [
        "# load the Fashion MNIST dataset\n",
        "print(\"[INFO] loading Fashion MNIST...\")\n",
        "((trainX, trainY), (testX, testY)) = fashion_mnist.load_data()\n",
        "\n",
        "# add a channel dimension to the dataset\n",
        "trainX = trainX.reshape((trainX.shape[0], 28, 28, 1))\n",
        "testX = testX.reshape((testX.shape[0], 28, 28, 1))\n",
        "\n",
        "# scale data to the range of [0, 1]\n",
        "trainX = trainX.astype(\"float32\") / 255.0\n",
        "testX = testX.astype(\"float32\") / 255.0\n",
        "\n",
        "# one-hot encode the training and testing labels\n",
        "trainY = to_categorical(trainY, 10)\n",
        "testY = to_categorical(testY, 10)\n",
        "\n",
        "# initialize the label names\n",
        "labelNames = [\"top\", \"trouser\", \"pullover\", \"dress\", \"coat\",\n",
        "\t\"sandal\", \"shirt\", \"sneaker\", \"bag\", \"ankle boot\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88MljWTG-rS6"
      },
      "source": [
        "# initialize an early stopping callback to prevent the model from\n",
        "# overfitting/spending too much time training with minimal gains\n",
        "es = EarlyStopping(\n",
        "\tmonitor=\"val_loss\",\n",
        "\tpatience=config.EARLY_STOPPING_PATIENCE,\n",
        "\trestore_best_weights=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1DYgCSb-ub-"
      },
      "source": [
        "# check if we will be using the hyperband tuner\n",
        "if args[\"tuner\"] == \"hyperband\":\n",
        "\t# instantiate the hyperband tuner object\n",
        "\tprint(\"[INFO] instantiating a hyperband tuner object...\")\n",
        "\ttuner = kt.Hyperband(\n",
        "\t\tbuild_model,\n",
        "\t\tobjective=\"val_accuracy\",\n",
        "\t\tmax_epochs=config.EPOCHS,\n",
        "\t\tfactor=3,\n",
        "\t\tseed=42,\n",
        "\t\tdirectory=config.OUTPUT_PATH,\n",
        "\t\tproject_name=args[\"tuner\"])\n",
        "\n",
        "# check if we will be using the random search tuner\n",
        "elif args[\"tuner\"] == \"random\":\n",
        "\t# instantiate the random search tuner object\n",
        "\tprint(\"[INFO] instantiating a random search tuner object...\")\n",
        "\ttuner = kt.RandomSearch(\n",
        "\t\tbuild_model,\n",
        "\t\tobjective=\"val_accuracy\",\n",
        "\t\tmax_trials=10,\n",
        "\t\tseed=42,\n",
        "\t\tdirectory=config.OUTPUT_PATH,\n",
        "\t\tproject_name=args[\"tuner\"])\n",
        "\n",
        "# otherwise, we will be using the bayesian optimization tuner\n",
        "else:\n",
        "\t# instantiate the bayesian optimization tuner object\n",
        "\tprint(\"[INFO] instantiating a bayesian optimization tuner object...\")\n",
        "\ttuner = kt.BayesianOptimization(\n",
        "\t\tbuild_model,\n",
        "\t\tobjective=\"val_accuracy\",\n",
        "\t\tmax_trials=10,\n",
        "\t\tseed=42,\n",
        "\t\tdirectory=config.OUTPUT_PATH,\n",
        "\t\tproject_name=args[\"tuner\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BcI4ORqF-8J7"
      },
      "source": [
        "# perform the hyperparameter search\n",
        "print(\"[INFO] performing hyperparameter search...\")\n",
        "tuner.search(\n",
        "\tx=trainX, y=trainY,\n",
        "\tvalidation_data=(testX, testY),\n",
        "\tbatch_size=config.BS,\n",
        "\tcallbacks=[es],\n",
        "\tepochs=config.EPOCHS\n",
        ")\n",
        "\n",
        "# grab the best hyperparameters\n",
        "bestHP = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "print(\"[INFO] optimal number of filters in conv_1 layer: {}\".format(\n",
        "\tbestHP.get(\"conv_1\")))\n",
        "print(\"[INFO] optimal number of filters in conv_2 layer: {}\".format(\n",
        "\tbestHP.get(\"conv_2\")))\n",
        "print(\"[INFO] optimal number of units in dense layer: {}\".format(\n",
        "\tbestHP.get(\"dense_units\")))\n",
        "print(\"[INFO] optimal learning rate: {:.4f}\".format(\n",
        "\tbestHP.get(\"learning_rate\")))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9l13P1IP_ElZ"
      },
      "source": [
        "# build the best model and train it\n",
        "print(\"[INFO] training the best model...\")\n",
        "model = tuner.hypermodel.build(bestHP)\n",
        "H = model.fit(x=trainX, y=trainY,\n",
        "\tvalidation_data=(testX, testY), batch_size=config.BS,\n",
        "\tepochs=config.EPOCHS, callbacks=[es], verbose=1)\n",
        "\n",
        "# evaluate the network\n",
        "print(\"[INFO] evaluating network...\")\n",
        "predictions = model.predict(x=testX, batch_size=32)\n",
        "print(classification_report(testY.argmax(axis=1),\n",
        "\tpredictions.argmax(axis=1), target_names=labelNames))\n",
        "\n",
        "# generate the training loss/accuracy plot\n",
        "save_plot(H, args[\"plot\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ogkNauArL6u"
      },
      "source": [
        "For a detailed walkthrough of the concepts and code, be sure to refer to the full tutorial, [*Easy Hyperparameter Tuning with Keras Tuner and TensorFlow*](https://www.pyimagesearch.com/2021/06/07/easy-hyperparameter-tuning-with-keras-tuner-and-tensorflow/) published on 2021-06-07."
      ]
    }
  ]
}