{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "507-video_augmented_reality.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/parekhakhil/pyImageSearch/blob/main/507_video_augmented_reality.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7uPgNc0NyO7"
      },
      "source": [
        "# OpenCV Video Augmented Reality\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mpthbjs-N2qJ"
      },
      "source": [
        "\n",
        "This notebook is associated with the [OpenCV Video Augmented Reality](https://www.pyimagesearch.com/2021/01/11/opencv-video-augmented-reality/) blog post published on 01-11-21.\n",
        "\n",
        "Only the code for the blog post is here. Most codeblocks have a 1:1 relationship with what you find in the blog post with two exceptions: (1) Python classes are not separate files as they are typically organized with PyImageSearch projects, and (2) Command Line Argument parsing is replaced with an `args` dictionary that you can manipulate as needed.\n",
        "\n",
        "We recommend that you execute (press ▶️) the code block-by-block, as-is, before adjusting parameters and `args` inputs. Once you've verified that the code is working, you are welcome to hack with it and learn from manipulating inputs, settings, and parameters. For more information on using Jupyter and Colab, please refer to these resources:\n",
        "\n",
        "*   [Jupyter Notebook User Interface](https://jupyter-notebook.readthedocs.io/en/stable/notebook.html#notebook-user-interface)\n",
        "*   [Overview of Google Colaboratory Features](https://colab.research.google.com/notebooks/basic_features_overview.ipynb)\n",
        "\n",
        " \n",
        "\n",
        "Happy hacking!\n",
        "\n",
        "\n",
        "<hr>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pL5rHGgOAMk"
      },
      "source": [
        "### Download the code zip file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPaVERZsMpT9"
      },
      "source": [
        "!wget https://pyimagesearch-code-downloads.s3-us-west-2.amazonaws.com/video-augmented-reality/video-augmented-reality.zip\n",
        "!unzip -qq video-augmented-reality.zip\n",
        "%cd video-augmented-reality"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yYRCnThOQXl"
      },
      "source": [
        "## Blog Post Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcmY4DDJOSIt"
      },
      "source": [
        "### Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlcPho8lOOuF"
      },
      "source": [
        "# import the necessary packages\n",
        "from collections import deque\n",
        "import numpy as np\n",
        "import argparse\n",
        "import imutils\n",
        "import time\n",
        "import cv2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3GUGlIHgi_y"
      },
      "source": [
        "### Implementing our marker detector/augmented reality utility function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M10Db6GmgzFZ"
      },
      "source": [
        "# initialize our cached reference points\n",
        "CACHED_REF_PTS = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFhI8PzHgkBY"
      },
      "source": [
        "def find_and_warp(frame, source, cornerIDs, arucoDict, arucoParams,\n",
        "\tuseCache=False):\n",
        "\t# grab a reference to our cached reference points\n",
        "\tglobal CACHED_REF_PTS\n",
        "\n",
        "\t# grab the width and height of the frame and source image,\n",
        "\t# respectively\n",
        "\t(imgH, imgW) = frame.shape[:2]\n",
        "\t(srcH, srcW) = source.shape[:2]\n",
        "\n",
        "\t# detect AruCo markers in the input frame\n",
        "\t(corners, ids, rejected) = cv2.aruco.detectMarkers(\n",
        "\t\tframe, arucoDict, parameters=arucoParams)\n",
        "\n",
        "\t# if we *did not* find our four ArUco markers, initialize an\n",
        "\t# empty IDs list, otherwise flatten the ID list\n",
        "\tids = np.array([]) if len(corners) != 4 else ids.flatten()\n",
        "\n",
        "\t# initialize our list of reference points\n",
        "\trefPts = []\n",
        "\n",
        "\t# loop over the IDs of the ArUco markers in top-left, top-right,\n",
        "\t# bottom-right, and bottom-left order\n",
        "\tfor i in cornerIDs:\n",
        "\t\t# grab the index of the corner with the current ID\n",
        "\t\tj = np.squeeze(np.where(ids == i))\n",
        "\n",
        "\t\t# if we receive an empty list instead of an integer index,\n",
        "\t\t# then we could not find the marker with the current ID\n",
        "\t\tif j.size == 0:\n",
        "\t\t\tcontinue\n",
        "\n",
        "\t\t# otherwise, append the corner (x, y)-coordinates to our list\n",
        "\t\t# of reference points\n",
        "\t\tcorner = np.squeeze(corners[j])\n",
        "\t\trefPts.append(corner)\n",
        "\n",
        "\t# check to see if we failed to find the four ArUco markers\n",
        "\tif len(refPts) != 4:\n",
        "\t\t# if we are allowed to use cached reference points, fall\n",
        "\t\t# back on them\n",
        "\t\tif useCache and CACHED_REF_PTS is not None:\n",
        "\t\t\trefPts = CACHED_REF_PTS\n",
        "\n",
        "\t\t# otherwise, we cannot use the cache and/or there are no\n",
        "\t\t# previous cached reference points, so return early\n",
        "\t\telse:\n",
        "\t\t\treturn None\n",
        "\n",
        "\t# if we are allowed to use cached reference points, then update\n",
        "\t# the cache with the current set\n",
        "\tif useCache:\n",
        "\t\tCACHED_REF_PTS = refPts\n",
        "\n",
        "\t# unpack our ArUco reference points and use the reference points\n",
        "\t# to define the *destination* transform matrix, making sure the\n",
        "\t# points are specified in top-left, top-right, bottom-right, and\n",
        "\t# bottom-left order\n",
        "\t(refPtTL, refPtTR, refPtBR, refPtBL) = refPts\n",
        "\tdstMat = [refPtTL[0], refPtTR[1], refPtBR[2], refPtBL[3]]\n",
        "\tdstMat = np.array(dstMat)\n",
        "\n",
        "\t# define the transform matrix for the *source* image in top-left,\n",
        "\t# top-right, bottom-right, and bottom-left order\n",
        "\tsrcMat = np.array([[0, 0], [srcW, 0], [srcW, srcH], [0, srcH]])\n",
        "\n",
        "\t# compute the homography matrix and then warp the source image to\n",
        "\t# the destination based on the homography\n",
        "\t(H, _) = cv2.findHomography(srcMat, dstMat)\n",
        "\twarped = cv2.warpPerspective(source, H, (imgW, imgH))\n",
        "\n",
        "\t# construct a mask for the source image now that the perspective\n",
        "\t# warp has taken place (we'll need this mask to copy the source\n",
        "\t# image into the destination)\n",
        "\tmask = np.zeros((imgH, imgW), dtype=\"uint8\")\n",
        "\tcv2.fillConvexPoly(mask, dstMat.astype(\"int32\"), (255, 255, 255),\n",
        "\t\tcv2.LINE_AA)\n",
        "\n",
        "\t# this step is optional, but to give the source image a black\n",
        "\t# border surrounding it when applied to the source image, you\n",
        "\t# can apply a dilation operation\n",
        "\trect = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))\n",
        "\tmask = cv2.dilate(mask, rect, iterations=2)\n",
        "\n",
        "\t# create a three channel version of the mask by stacking it\n",
        "\t# depth-wise, such that we can copy the warped source image\n",
        "\t# into the input image\n",
        "\tmaskScaled = mask.copy() / 255.0\n",
        "\tmaskScaled = np.dstack([maskScaled] * 3)\n",
        "\n",
        "\t# copy the warped source image into the input image by\n",
        "\t# (1) multiplying the warped image and masked together,\n",
        "\t# (2) then multiplying the original input image with the\n",
        "\t# mask (giving more weight to the input where there\n",
        "\t# *ARE NOT* masked pixels), and (3) adding the resulting\n",
        "\t# multiplications together\n",
        "\twarpedMultiplied = cv2.multiply(warped.astype(\"float\"),\n",
        "\t\tmaskScaled)\n",
        "\timageMultiplied = cv2.multiply(frame.astype(float),\n",
        "\t\t1.0 - maskScaled)\n",
        "\toutput = cv2.add(warpedMultiplied, imageMultiplied)\n",
        "\toutput = output.astype(\"uint8\")\n",
        "\n",
        "\t# return the output frame to the calling function\n",
        "\treturn output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZQyf5hKQlZy"
      },
      "source": [
        "### Creating our OpenCV video augmented reality driver script"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXTA_gnY8jiO"
      },
      "source": [
        "# first, let's download a sample video we will use to detect ArUco markers\n",
        "!wget https://colab-notebook-videos.s3-us-west-2.amazonaws.com/aruco_sample.mp4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hf2XwSgQmsj"
      },
      "source": [
        "# construct the argument parser and parse the arguments\n",
        "#ap = argparse.ArgumentParser()\n",
        "#ap.add_argument(\"-i\", \"--input\", type=str, required=True,\n",
        "#\thelp=\"path to input video file for augmented reality\")\n",
        "#ap.add_argument(\"-c\", \"--cache\", type=int, default=-1,\n",
        "#\thelp=\"whether or not to use reference points cache\")\n",
        "#args = vars(ap.parse_args())\n",
        "\n",
        "# since we are using Jupyter Notebooks we can replace our argument\n",
        "# parsing code with *hard coded* arguments and values\n",
        "args = {\n",
        "    \"input\": \"videos/jp_trailer_short.mp4\",\n",
        "    \"cache\": -1,\n",
        "    \"video\": \"aruco_sample.mp4\",\n",
        "    \"output\": \"output.avi\"\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zc5-hGHgRXsq"
      },
      "source": [
        "# load the ArUCo dictionary and grab the ArUCo parameters\n",
        "print(\"[INFO] initializing marker detector...\")\n",
        "arucoDict = cv2.aruco.Dictionary_get(cv2.aruco.DICT_ARUCO_ORIGINAL)\n",
        "arucoParams = cv2.aruco.DetectorParameters_create()\n",
        "\n",
        "# initialize the video file stream\n",
        "print(\"[INFO] accessing video stream...\")\n",
        "vf = cv2.VideoCapture(args[\"input\"])\n",
        "\n",
        "# initialize a queue to maintain the next frame from the video stream\n",
        "Q = deque(maxlen=128)\n",
        "\n",
        "# we need to have a frame in our queue to start our augmented reality\n",
        "# pipeline, so read the next frame from our video file source and add\n",
        "# it to our queue\n",
        "(grabbed, source) = vf.read()\n",
        "Q.appendleft(source)\n",
        "\n",
        "# initialize the video stream and initialize pointer to output\n",
        "# video file\n",
        "print(\"[INFO] starting video stream...\")\n",
        "vs = cv2.VideoCapture(args[\"video\"])\n",
        "writer = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHIF5c9nT_Ck"
      },
      "source": [
        "# loop over the frames from the video stream\n",
        "while len(Q) > 0:\n",
        "\t# grab the frame from our video stream\n",
        "\tframe = vs.read()[1]\n",
        "\n",
        "    # if we did not grab a frame then we have reached the end of the\n",
        "\t# video\n",
        "\tif frame is None:\n",
        "\t\tbreak\n",
        "\n",
        "    # resize the frame to have a maximum width of 600 pixels\n",
        "\tframe = imutils.resize(frame, width=600)\n",
        "\n",
        "\t# attempt to find the ArUCo markers in the frame, and provided\n",
        "\t# they are found, take the current source image and warp it onto\n",
        "\t# input frame using our augmented reality technique\n",
        "\twarped = find_and_warp(\n",
        "\t\tframe, source,\n",
        "\t\tcornerIDs=(923, 1001, 241, 1007),\n",
        "\t\tarucoDict=arucoDict,\n",
        "\t\tarucoParams=arucoParams,\n",
        "\t\tuseCache=args[\"cache\"] > 0)\n",
        "\n",
        "\t# if the warped frame is not None, then we know (1) we found the\n",
        "\t# four ArUCo markers and (2) the perspective warp was successfully\n",
        "\t# applied\n",
        "\tif warped is not None:\n",
        "\t\t# set the frame to the output augment reality frame and then\n",
        "\t\t# grab the next video file frame from our queue\n",
        "\t\tframe = warped\n",
        "\t\tsource = Q.popleft()\n",
        "\n",
        "\t# for speed/efficiency, we can use a queue to keep the next video\n",
        "\t# frame queue ready for us -- the trick is to ensure the queue is\n",
        "\t# always (or nearly full)\n",
        "\tif len(Q) != Q.maxlen:\n",
        "\t\t# read the next frame from the video file stream\n",
        "\t\t(grabbed, nextFrame) = vf.read()\n",
        "\n",
        "\t\t# if the frame was read (meaning we are not at the end of the\n",
        "\t\t# video file stream), add the frame to our queue\n",
        "\t\tif grabbed:\n",
        "\t\t\tQ.append(nextFrame)\n",
        "\n",
        "    # if the video writer is None *AND* we are supposed to write\n",
        "\t# the output video to disk initialize the writer\n",
        "\tif writer is None and args[\"output\"] is not None:\n",
        "\t\tfourcc = cv2.VideoWriter_fourcc(*\"MJPG\")\n",
        "\t\twriter = cv2.VideoWriter(args[\"output\"], fourcc, 20,\n",
        "\t\t\t(frame.shape[1], frame.shape[0]), True)\n",
        "\n",
        "\t# if the writer is not None, write the frame to disk\n",
        "\tif writer is not None:\n",
        "\t\twriter.write(frame)\n",
        "\n",
        "# do a bit of cleanup\n",
        "vs.release()\n",
        "vf.release()\n",
        "\n",
        "# check to see if the video writer point needs to be released\n",
        "if writer is not None:\n",
        "\twriter.release()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTmKI1XuZ4Xa"
      },
      "source": [
        "The above code block takes a while to complete its execution. If you are interested to view the video within Colab just execute the following code blocks. Note that it may be time-consuming. \n",
        "\n",
        "Our output video is produced in `.avi` format. First, we need to convert it to `.mp4` format. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_BU3eRoVgvY"
      },
      "source": [
        "!ffmpeg -i \"output.avi\" output.mp4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7ZUEduqZQnq",
        "cellView": "form"
      },
      "source": [
        "#@title Display video inline\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "mp4 = open(\"output.mp4\", \"rb\").read()\n",
        "dataURL = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(\"\"\"\n",
        "<video width=700 controls>\n",
        "      <source src=\"%s\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\" % dataURL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcXyaa3EZ72O"
      },
      "source": [
        "This code is referred from [this StackOverflow thread](https://stackoverflow.com/a/57378660/7636462). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIK0y_52aETm"
      },
      "source": [
        "For a detailed walkthrough of the concepts and code, be sure to refer to the full tutorial, [*OpenCV Video Augmented Reality*](https://www.pyimagesearch.com/2021/01/11/opencv-video-augmented-reality/) blog post published on 01-11-2021."
      ]
    }
  ]
}