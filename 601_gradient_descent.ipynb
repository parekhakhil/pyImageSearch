{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "601-gradient_descent.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/parekhakhil/pyImageSearch/blob/main/601_gradient_descent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4RpK9pawQzP"
      },
      "source": [
        "# Gradient descent with Python\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ntZ1AkXZIxY"
      },
      "source": [
        "\n",
        "This notebook is associated with the [Gradient descent with Python](https://www.pyimagesearch.com/2016/10/10/gradient-descent-with-python/) blog post published on 2016-10-10.\n",
        "\n",
        "Only the code for the blog post is here. Most codeblocks have a 1:1 relationship with what you find in the blog post with two exceptions: (1) Python classes are not separate files as they are typically organized with PyImageSearch projects, and (2) Command Line Argument parsing is replaced with an `args` dictionary that you can manipulate as needed.\n",
        "\n",
        "We recommend that you execute (press ▶️) the code block-by-block, as-is, before adjusting parameters and `args` inputs. Once you've verified that the code is working, you are welcome to hack with it and learn from manipulating inputs, settings, and parameters. For more information on using Jupyter and Colab, please refer to these resources:\n",
        "\n",
        "*   [Jupyter Notebook User Interface](https://jupyter-notebook.readthedocs.io/en/stable/notebook.html#notebook-user-interface)\n",
        "*   [Overview of Google Colaboratory Features](https://colab.research.google.com/notebooks/basic_features_overview.ipynb)\n",
        "\n",
        "\n",
        "Happy hacking!\n",
        "\n",
        "\n",
        "\n",
        "<hr>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFhAzQB3aNMa"
      },
      "source": [
        "### Download the code zip file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7y0LG1EuaRlB"
      },
      "source": [
        "!wget https://www.pyimagesearch.com/wp-content/uploads/2016/08/gradient-descent.zip\n",
        "!unzip -qq gradient-descent.zip\n",
        "%cd gradient-descent"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SgTVT3HagGZ"
      },
      "source": [
        "## Blog Post Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcrOk6pURp50"
      },
      "source": [
        "### Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJaCNlDDRz6d"
      },
      "source": [
        "# import the necessary packages\n",
        "from sklearn.datasets.samples_generator import make_blobs\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import argparse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBHxDn2YBCVp"
      },
      "source": [
        "### Implementing gradient descent with Python\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6qnpHoCAgdn"
      },
      "source": [
        "def sigmoid_activation(x):\n",
        "\t# compute and return the sigmoid activation value for a\n",
        "\t# given input value\n",
        "\treturn 1.0 / (1 + np.exp(-x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnurPvAVBJ4c"
      },
      "source": [
        "# construct the argument parse and parse the arguments\n",
        "# ap = argparse.ArgumentParser()\n",
        "# ap.add_argument(\"-e\", \"--epochs\", type=float, default=100,\n",
        "# \thelp=\"# of epochs\")\n",
        "# ap.add_argument(\"-a\", \"--alpha\", type=float, default=0.01,\n",
        "# \thelp=\"learning rate\")\n",
        "# args = vars(ap.parse_args())\n",
        "\n",
        "# since we are using Jupyter Notebooks we can replace our argument\n",
        "# parsing code with *hard coded* arguments and values\n",
        "args = {\n",
        "\t\"epochs\": 100,\n",
        "\t\"alpha\": 0.01\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AxfRQkEBRid"
      },
      "source": [
        "# generate a 2-class classification problem with 250 data points,\n",
        "# where each data point is a 2D feature vector\n",
        "(X, y) = make_blobs(n_samples=250, n_features=2, centers=2,\n",
        "\tcluster_std=1.05, random_state=20)\n",
        "\n",
        "# insert a column of 1's as the first entry in the feature\n",
        "# vector -- this is a little trick that allows us to treat\n",
        "# the bias as a trainable parameter *within* the weight matrix\n",
        "# rather than an entirely separate variable\n",
        "X = np.c_[np.ones((X.shape[0])), X]\n",
        "\n",
        "# initialize our weight matrix such it has the same number of\n",
        "# columns as our input features\n",
        "print(\"[INFO] starting training...\")\n",
        "W = np.random.uniform(size=(X.shape[1],))\n",
        "\n",
        "# initialize a list to store the loss value for each epoch\n",
        "lossHistory = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xt_CiZrDBV7N"
      },
      "source": [
        "# loop over the desired number of epochs\n",
        "for epoch in np.arange(0, args[\"epochs\"]):\n",
        "\t# take the dot product between our features `X` and the\n",
        "\t# weight matrix `W`, then pass this value through the\n",
        "\t# sigmoid activation function, thereby giving us our\n",
        "\t# predictions on the dataset\n",
        "\tpreds = sigmoid_activation(X.dot(W))\n",
        "\n",
        "\t# now that we have our predictions, we need to determine\n",
        "\t# our `error`, which is the difference between our predictions\n",
        "\t# and the true values\n",
        "\terror = preds - y\n",
        "\n",
        "\t# given our `error`, we can compute the total loss value as\n",
        "\t# the sum of squared loss -- ideally, our loss should\n",
        "\t# decrease as we continue training\n",
        "\tloss = np.sum(error ** 2)\n",
        "\tlossHistory.append(loss)\n",
        "\tprint(\"[INFO] epoch #{}, loss={:.7f}\".format(epoch + 1, loss))\n",
        "\n",
        "\t# the gradient update is therefore the dot product between\n",
        "\t# the transpose of `X` and our error, scaled by the total\n",
        "\t# number of data points in `X`\n",
        "\tgradient = X.T.dot(error) / X.shape[0]\n",
        "\n",
        "\t# in the update stage, all we need to do is nudge our weight\n",
        "\t# matrix in the opposite direction of the gradient (hence the\n",
        "\t# term \"gradient descent\" by taking a small step towards a\n",
        "\t# set of \"more optimal\" parameters\n",
        "\tW += -args[\"alpha\"] * gradient"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6qEezI6BdjW"
      },
      "source": [
        "# to demonstrate how to use our weight matrix as a classifier,\n",
        "# let's look over our a sample of training examples\n",
        "for i in np.random.choice(250, 10):\n",
        "\t# compute the prediction by taking the dot product of the\n",
        "\t# current feature vector with the weight matrix W, then\n",
        "\t# passing it through the sigmoid activation function\n",
        "\tactivation = sigmoid_activation(X[i].dot(W))\n",
        "\n",
        "\t# the sigmoid function is defined over the range y=[0, 1],\n",
        "\t# so we can use 0.5 as our threshold -- if `activation` is\n",
        "\t# below 0.5, it's class `0`; otherwise it's class `1`\n",
        "\tlabel = 0 if activation < 0.5 else 1\n",
        "\n",
        "\t# show our output classification\n",
        "\tprint(\"activation={:.4f}; predicted_label={}, true_label={}\".format(\n",
        "\t\tactivation, label, y[i]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2swUTHgBgP3"
      },
      "source": [
        "# compute the line of best fit by setting the sigmoid function\n",
        "# to 0 and solving for X2 in terms of X1\n",
        "Y = (-W[0] - (W[1] * X)) / W[2]\n",
        "\n",
        "# plot the original data along with our line of best fit\n",
        "plt.figure()\n",
        "plt.scatter(X[:, 1], X[:, 2], marker=\"o\", c=y)\n",
        "plt.plot(X, Y, \"r-\")\n",
        "\n",
        "# construct a figure that plots the loss over time\n",
        "fig = plt.figure()\n",
        "plt.plot(np.arange(0, args[\"epochs\"]), lossHistory)\n",
        "fig.suptitle(\"Training Loss\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ogkNauArL6u"
      },
      "source": [
        "For a detailed walkthrough of the concepts and code, be sure to refer to the full tutorial, [*Gradient descent with Python*](https://www.pyimagesearch.com/2016/10/10/gradient-descent-with-python/) blog post published on 2016-10-10."
      ]
    }
  ]
}