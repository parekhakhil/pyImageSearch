{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of understanding_regularization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/parekhakhil/pyImageSearch/blob/main/604-understanding_regularization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4RpK9pawQzP"
      },
      "source": [
        "# Understanding regularization for image classification and machine learning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ntZ1AkXZIxY"
      },
      "source": [
        "\n",
        "This notebook is associated with the [Understanding regularization for image classification and machine learning](https://www.pyimagesearch.com/2016/09/19/understanding-regularization-for-image-classification-and-machine-learning/) blog post published on 2016-09-19.\n",
        "\n",
        "Only the code for the blog post is here. Most codeblocks have a 1:1 relationship with what you find in the blog post with two exceptions: (1) Python classes are not separate files as they are typically organized with PyImageSearch projects, and (2) Command Line Argument parsing is replaced with an `args` dictionary that you can manipulate as needed.\n",
        "\n",
        "We recommend that you execute (press ▶️) the code block-by-block, as-is, before adjusting parameters and `args` inputs. Once you've verified that the code is working, you are welcome to hack with it and learn from manipulating inputs, settings, and parameters. For more information on using Jupyter and Colab, please refer to these resources:\n",
        "\n",
        "*   [Jupyter Notebook User Interface](https://jupyter-notebook.readthedocs.io/en/stable/notebook.html#notebook-user-interface)\n",
        "*   [Overview of Google Colaboratory Features](https://colab.research.google.com/notebooks/basic_features_overview.ipynb)\n",
        "\n",
        "\n",
        "Happy hacking!\n",
        "\n",
        "\n",
        "<hr>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFhAzQB3aNMa"
      },
      "source": [
        "### Download the code zip file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7y0LG1EuaRlB"
      },
      "source": [
        "!wget https://www.pyimagesearch.com/wp-content/uploads/2016/08/understanding-regularization.zip\n",
        "!unzip -qq understanding-regularization.zip\n",
        "%cd understanding-regularization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdnSNKcTWjX0"
      },
      "source": [
        "### Downloading the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1_CKjreWlyN"
      },
      "source": [
        "Since this dataset is hosted on Kaggle, we have a few options to get the dataset from Kaggle to our Colab Notebook environment - \n",
        "\n",
        "* Download the dataset from Kaggle as a zipped file, upload that to our Google Drive, and then mount Google Drive on Colab to access the uploaded dataset. \n",
        "* Use the [Kaggle API](https://github.com/Kaggle/kaggle-api) to directly download the dataset to our Colab Notebook environment. \n",
        "\n",
        "We will be using the second option. **Note** that you need to obtain your Kaggle API keys to perform this step. Follow the instructions [here](https://github.com/Kaggle/kaggle-api) in order to obtain your Kaggle API keys in case you don't have them. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQzkntpLYHqd"
      },
      "source": [
        "Now, navigate to File Browser of Colab and upload your keys following [this screencast](https://www.loom.com/share/ca76bb983e0844e2a7f14b473d7287c6). After the keys file has been uploaded, we need to move it to an appropriate location. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwoCfJkFZWue"
      },
      "source": [
        "!mkdir ~/.kaggle\n",
        "!cp /content/kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7-1bQrzZjDc"
      },
      "source": [
        "Now, we can download the dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gaM8kW3uZl7u"
      },
      "source": [
        "!kaggle competitions download -c dogs-vs-cats -f train.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tu58nbppaKe8"
      },
      "source": [
        "To follow along with the rest of the code, please move the download dataset by executing the command below. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2H3ZkkJaQi9"
      },
      "source": [
        "!mkdir kaggle_dogs_vs_cats\n",
        "!unzip --qq train.zip -d kaggle_dogs_vs_cats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SgTVT3HagGZ"
      },
      "source": [
        "## Blog Post Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcrOk6pURp50"
      },
      "source": [
        "### Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJaCNlDDRz6d"
      },
      "source": [
        "# import the necessary packages\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imutils import paths\n",
        "import numpy as np\n",
        "import argparse\n",
        "import imutils\n",
        "import cv2\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jppw5-Bd56H-"
      },
      "source": [
        "### Image classification using regularization with Python and scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdrJC3g7GwAQ"
      },
      "source": [
        "def extract_color_histogram(image, bins=(8, 8, 8)):\n",
        "\t# extract a 3D color histogram from the HSV color space using\n",
        "\t# the supplied number of `bins` per channel\n",
        "\thsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
        "\thist = cv2.calcHist([hsv], [0, 1, 2], None, bins,\n",
        "\t\t[0, 180, 0, 256, 0, 256])\n",
        "\n",
        "\t# handle normalizing the histogram if we are using OpenCV 2.4.X\n",
        "\tif imutils.is_cv2():\n",
        "\t\thist = cv2.normalize(hist)\n",
        "\n",
        "\t# otherwise, perform \"in place\" normalization in OpenCV 3 (I\n",
        "\t# personally hate the way this is done\n",
        "\telse:\n",
        "\t\tcv2.normalize(hist, hist)\n",
        "\n",
        "\t# return the flattened histogram as the feature vector\n",
        "\treturn hist.flatten()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okM7Bpyeq8Kc"
      },
      "source": [
        "# construct the argument parser and parse the arguments\n",
        "# ap = argparse.ArgumentParser()\n",
        "# ap.add_argument(\"-d\", \"--dataset\", required=True,\n",
        "# \thelp=\"path to input dataset\")\n",
        "# args = vars(ap.parse_args())\n",
        "\n",
        "# since we are using Jupyter Notebooks we can replace our argument\n",
        "# parsing code with *hard coded* arguments and values\n",
        "args = {\n",
        "\t\"dataset\": \"kaggle_dogs_vs_cats\"\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGPAA5nLG6Kp"
      },
      "source": [
        "# grab the list of images that we'll be describing\n",
        "print(\"[INFO] describing images...\")\n",
        "imagePaths = list(paths.list_images(args[\"dataset\"]))\n",
        "\n",
        "# initialize the data matrix and labels list\n",
        "data = []\n",
        "labels = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckR9p2_oG6Hk"
      },
      "source": [
        "# loop over the input images\n",
        "for (i, imagePath) in enumerate(imagePaths):\n",
        "\t# load the image and extract the class label (assuming that our\n",
        "\t# path as the format: /path/to/dataset/{class}.{image_num}.jpg\n",
        "\timage = cv2.imread(imagePath)\n",
        "\tlabel = imagePath.split(os.path.sep)[-1].split(\".\")[0]\n",
        "\n",
        "\t# extract a color histogram from the image, then update the\n",
        "\t# data matrix and labels list\n",
        "\thist = extract_color_histogram(image)\n",
        "\tdata.append(hist)\n",
        "\tlabels.append(label)\n",
        "\n",
        "\t# show an update every 1,000 images\n",
        "\tif i > 0 and i % 1000 == 0:\n",
        "\t\tprint(\"[INFO] processed {}/{}\".format(i, len(imagePaths)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5plF6rIHI8e"
      },
      "source": [
        "# encode the labels, converting them from strings to integers\n",
        "le = LabelEncoder()\n",
        "labels = le.fit_transform(labels)\n",
        "\n",
        "# partition the data into training and testing splits, using 75%\n",
        "# of the data for training and the remaining 25% for testing\n",
        "print(\"[INFO] constructing training/testing split...\")\n",
        "(trainData, testData, trainLabels, testLabels) = train_test_split(\n",
        "\tnp.array(data), labels, test_size=0.25, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJhp3-7MHI5o"
      },
      "source": [
        "# loop over our set of regularizers\n",
        "for r in (None, \"l1\", \"l2\", \"elasticnet\"):\n",
        "\t# train a Stochastic Gradient Descent classifier using a softmax\n",
        "\t# loss function, the specified regularizer, and 10 epochs\n",
        "\tprint(\"[INFO] training model with `{}` penalty\".format(r))\n",
        "\tmodel = SGDClassifier(loss=\"log\", penalty=r, random_state=967,\n",
        "\t\tmax_iter=10)\n",
        "\tmodel.fit(trainData, trainLabels)\n",
        "\n",
        "\t# evaluate the classifier\n",
        "\tacc = model.score(testData, testLabels)\n",
        "\tprint(\"[INFO] `{}` penalty accuracy: {:.2f}%\".format(r, acc * 100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ogkNauArL6u"
      },
      "source": [
        "For a detailed walkthrough of the concepts and code, be sure to refer to the full tutorial, [*Understanding regularization for image classification and machine learning*](https://www.pyimagesearch.com/2016/09/19/understanding-regularization-for-image-classification-and-machine-learning/) published on 2016-09-19."
      ]
    }
  ]
}
