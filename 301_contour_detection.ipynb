{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "301-contour_detection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/parekhakhil/pyImageSearch/blob/main/301_contour_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7uPgNc0NyO7"
      },
      "source": [
        "# OpenCV Connected Component Labeling and Analysis\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mpthbjs-N2qJ"
      },
      "source": [
        "\n",
        "This notebook is associated with the [OpenCV Connected Component Labeling and Analysis](https://www.pyimagesearch.com/2021/02/22/opencv-connected-component-labeling-and-analysis/) blog post published on 02-22-21.\n",
        "\n",
        "Only the code for the blog post is here. Most codeblocks have a 1:1 relationship with what you find in the blog post with two exceptions: (1) Python classes are not separate files as they are typically organized with PyImageSearch projects, and (2) Command Line Argument parsing is replaced with an `args` dictionary that you can manipulate as needed.\n",
        "\n",
        "We recommend that you execute (press ▶️) the code block-by-block, as-is, before adjusting parameters and `args` inputs. Once you've verified that the code is working, you are welcome to hack with it and learn from manipulating inputs, settings, and parameters. For more information on using Jupyter and Colab, please refer to these resources:\n",
        "\n",
        "*   [Jupyter Notebook User Interface](https://jupyter-notebook.readthedocs.io/en/stable/notebook.html#notebook-user-interface)\n",
        "*   [Overview of Google Colaboratory Features](https://colab.research.google.com/notebooks/basic_features_overview.ipynb)\n",
        "\n",
        "\n",
        "\n",
        "Happy hacking!\n",
        "\n",
        "\n",
        "<hr>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pL5rHGgOAMk"
      },
      "source": [
        "### Download the code zip file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEZbwcwepI9V"
      },
      "source": [
        "!wget https://pyimagesearch-code-downloads.s3-us-west-2.amazonaws.com/opencv-connected-components/opencv-connected-components.zip\n",
        "!unzip -qq opencv-connected-components.zip\n",
        "%cd opencv-connected-components"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yYRCnThOQXl"
      },
      "source": [
        "## Blog Post Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcmY4DDJOSIt"
      },
      "source": [
        "### Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlcPho8lOOuF"
      },
      "source": [
        "# import the necessary packages\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import argparse\n",
        "import cv2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uY1ceK63OuFY"
      },
      "source": [
        "### Function to display images in Jupyter Notebooks and Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkPDb7emOuwQ"
      },
      "source": [
        "def plt_imshow(title, image):\n",
        "    # convert the image frame BGR to RGB color space and display it\n",
        "\timage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\tplt.imshow(image)\n",
        "\tplt.title(title)\n",
        "\tplt.grid(False)\n",
        "\tplt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IaUTUAPB3nmc"
      },
      "source": [
        "### Implementing basic connected components with OpenCV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24Jz3DYG30tM"
      },
      "source": [
        "# # construct the argument parser and parse the arguments\n",
        "# ap = argparse.ArgumentParser()\n",
        "# ap.add_argument(\"-i\", \"--image\", required=True,\n",
        "# \thelp=\"path to input image\")\n",
        "# ap.add_argument(\"-c\", \"--connectivity\", type=int, default=4,\n",
        "# \thelp=\"connectivity for connected component analysis\")\n",
        "# args = vars(ap.parse_args())\n",
        "\n",
        "# since we are using Jupyter Notebooks we can replace our argument\n",
        "# parsing code with *hard coded* arguments and values\n",
        "args = {\n",
        "    \"image\": \"license_plate.png\",\n",
        "    \"connectivity\": 4\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQkuPOK24CN7"
      },
      "source": [
        "# load the input image from disk, convert it to grayscale, and\n",
        "# threshold it\n",
        "image = cv2.imread(args[\"image\"])\n",
        "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "thresh = cv2.threshold(gray, 0, 255,\n",
        "\tcv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsj1N4o64I8-"
      },
      "source": [
        "# apply connected component analysis to the thresholded image\n",
        "output = cv2.connectedComponentsWithStats(\n",
        "\tthresh, args[\"connectivity\"], cv2.CV_32S)\n",
        "(numLabels, labels, stats, centroids) = output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4iZR7c7S4OEQ"
      },
      "source": [
        "# loop over the number of unique connected component labels\n",
        "for i in range(0, numLabels):\n",
        "\t# if this is the first component then we examining the\n",
        "\t# *background* (typically we would just ignore this\n",
        "\t# component in our loop)\n",
        "\tif i == 0:\n",
        "\t\ttext = \"examining component {}/{} (background)\".format(\n",
        "\t\t\ti + 1, numLabels)\n",
        "\n",
        "\t# otherwise, we are examining an actual connected component\n",
        "\telse:\n",
        "\t\ttext = \"examining component {}/{}\".format( i + 1, numLabels)\n",
        "\n",
        "\t# print a status message update for the current connected\n",
        "\t# component\n",
        "\tprint(\"[INFO] {}\".format(text))\n",
        "\n",
        "\t# extract the connected component statistics and centroid for\n",
        "\t# the current label\n",
        "\tx = stats[i, cv2.CC_STAT_LEFT]\n",
        "\ty = stats[i, cv2.CC_STAT_TOP]\n",
        "\tw = stats[i, cv2.CC_STAT_WIDTH]\n",
        "\th = stats[i, cv2.CC_STAT_HEIGHT]\n",
        "\tarea = stats[i, cv2.CC_STAT_AREA]\n",
        "\t(cX, cY) = centroids[i]\n",
        "\n",
        "\t# clone our original image (so we can draw on it) and then draw\n",
        "\t# a bounding box surrounding the connected component along with\n",
        "\t# a circle corresponding to the centroid\n",
        "\toutput = image.copy()\n",
        "\tcv2.rectangle(output, (x, y), (x + w, y + h), (0, 255, 0), 3)\n",
        "\tcv2.circle(output, (int(cX), int(cY)), 4, (0, 0, 255), -1)\n",
        "\n",
        "\t# construct a mask for the current connected component by\n",
        "\t# finding a pixels in the labels array that have the current\n",
        "\t# connected component ID\n",
        "\tcomponentMask = (labels == i).astype(\"uint8\") * 255\n",
        "\n",
        "\t# show our output image and connected component mask\n",
        "\tplt_imshow(\"Output\", output)\n",
        "\tplt_imshow(\"Connected Component\", componentMask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evdoY3Ps4giL"
      },
      "source": [
        "### How to filter connected components with OpenCV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rf-Lps_k4jjd"
      },
      "source": [
        "# # construct the argument parser and parse the arguments\n",
        "# ap = argparse.ArgumentParser()\n",
        "# ap.add_argument(\"-i\", \"--image\", required=True,\n",
        "# \thelp=\"path to input image\")\n",
        "# ap.add_argument(\"-c\", \"--connectivity\", type=int, default=4,\n",
        "# \thelp=\"connectivity for connected component analysis\")\n",
        "# args = vars(ap.parse_args())\n",
        "\n",
        "# since we are using Jupyter Notebooks we can replace our argument\n",
        "# parsing code with *hard coded* arguments and values\n",
        "args = {\n",
        "    \"image\": \"license_plate.png\",\n",
        "    \"connectivity\": 4\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6USxSJwn4vwl"
      },
      "source": [
        "# load the input image from disk, convert it to grayscale, and\n",
        "# threshold it\n",
        "image = cv2.imread(args[\"image\"])\n",
        "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "thresh = cv2.threshold(gray, 0, 255,\n",
        "\tcv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)[1]\n",
        "\n",
        "# apply connected component analysis to the thresholded image\n",
        "output = cv2.connectedComponentsWithStats(\n",
        "\tthresh, args[\"connectivity\"], cv2.CV_32S)\n",
        "(numLabels, labels, stats, centroids) = output\n",
        "\n",
        "# initialize an output mask to store all characters parsed from\n",
        "# the license plate\n",
        "mask = np.zeros(gray.shape, dtype=\"uint8\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYWsbD2Q4zTf"
      },
      "source": [
        "# loop over the number of unique connected component labels, skipping\n",
        "# over the first label (as label zero is the background)\n",
        "for i in range(1, numLabels):\n",
        "\t# extract the connected component statistics for the current\n",
        "\t# label\n",
        "\tx = stats[i, cv2.CC_STAT_LEFT]\n",
        "\ty = stats[i, cv2.CC_STAT_TOP]\n",
        "\tw = stats[i, cv2.CC_STAT_WIDTH]\n",
        "\th = stats[i, cv2.CC_STAT_HEIGHT]\n",
        "\tarea = stats[i, cv2.CC_STAT_AREA]\n",
        "\n",
        "\t# ensure the width, height, and area are all neither too small\n",
        "\t# nor too big\n",
        "\tkeepWidth = w > 5 and w < 50\n",
        "\tkeepHeight = h > 45 and h < 65\n",
        "\tkeepArea = area > 500 and area < 1500\n",
        "\n",
        "\t# ensure the connected component we are examining passes all\n",
        "\t# three tests\n",
        "\tif all((keepWidth, keepHeight, keepArea)):\n",
        "\t\t# construct a mask for the current connected component and\n",
        "\t\t# then take the bitwise OR with the mask\n",
        "\t\tprint(\"[INFO] keeping connected component {}\".format(i))\n",
        "\t\tcomponentMask = (labels == i).astype(\"uint8\") * 255\n",
        "\t\tmask = cv2.bitwise_or(mask, componentMask)\n",
        "\n",
        "# show the original input image and the mask for the license plate\n",
        "# characters\n",
        "plt_imshow(\"Image\", image)\n",
        "plt_imshow(\"Characters\", mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIK0y_52aETm"
      },
      "source": [
        "For a detailed walkthrough of the concepts and code, be sure to refer to the full tutorial, [*OpenCV Connected Component Labeling and Analysis*](https://www.pyimagesearch.com/2021/02/22/opencv-connected-component-labeling-and-analysis/) published on 02-22-21.\n",
        "\n",
        "---\n",
        "\n"
      ]
    }
  ]
}