{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "902-keras_regression_cnns.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/parekhakhil/pyImageSearch/blob/main/902_keras_regression_cnns.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4RpK9pawQzP"
      },
      "source": [
        "# Keras, Regression, and CNNs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ntZ1AkXZIxY"
      },
      "source": [
        "\n",
        "This notebook is associated with the [Keras, Regression, and CNNs](https://www.pyimagesearch.com/2019/01/28/keras-regression-and-cnns/) blog post published on 2019-01-28.\n",
        "\n",
        "Only the code for the blog post is here. Most codeblocks have a 1:1 relationship with what you find in the blog post with two exceptions: (1) Python classes are not separate files as they are typically organized with PyImageSearch projects, and (2) Command Line Argument parsing is replaced with an `args` dictionary that you can manipulate as needed.\n",
        "\n",
        "We recommend that you execute (press ▶️) the code block-by-block, as-is, before adjusting parameters and `args` inputs. Once you've verified that the code is working, you are welcome to hack with it and learn from manipulating inputs, settings, and parameters. For more information on using Jupyter and Colab, please refer to these resources:\n",
        "\n",
        "*   [Jupyter Notebook User Interface](https://jupyter-notebook.readthedocs.io/en/stable/notebook.html#notebook-user-interface)\n",
        "*   [Overview of Google Colaboratory Features](https://colab.research.google.com/notebooks/basic_features_overview.ipynb)\n",
        "\n",
        "\n",
        "\n",
        "Happy hacking!\n",
        "\n",
        "\n",
        "<hr>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFhAzQB3aNMa"
      },
      "source": [
        "### Download the code zip file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7y0LG1EuaRlB"
      },
      "source": [
        "!wget https://s3-us-west-2.amazonaws.com/static.pyimagesearch.com/keras-regression-cnns/keras-regression-cnns.zip\n",
        "!unzip -qq keras-regression-cnns.zip\n",
        "%cd keras-regression-cnns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtB164Pkw8Rj"
      },
      "source": [
        "### Downloading the House Prices Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sl-qEDgzw9kq"
      },
      "source": [
        "!git clone https://github.com/emanhamed/Houses-dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SgTVT3HagGZ"
      },
      "source": [
        "## Blog Post Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcrOk6pURp50"
      },
      "source": [
        "### Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgVb0qKp1sml"
      },
      "source": [
        "# import the necessary packages\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import argparse\n",
        "import locale\n",
        "import glob\n",
        "import cv2\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHE0ryRk16RI"
      },
      "source": [
        "### Loading the house prices image dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHydGovp3FSx"
      },
      "source": [
        "def load_house_attributes(inputPath):\n",
        "\t# initialize the list of column names in the CSV file and then\n",
        "\t# load it using Pandas\n",
        "\tcols = [\"bedrooms\", \"bathrooms\", \"area\", \"zipcode\", \"price\"]\n",
        "\tdf = pd.read_csv(inputPath, sep=\" \", header=None, names=cols)\n",
        "\n",
        "\t# determine (1) the unique zip codes and (2) the number of data\n",
        "\t# points with each zip code\n",
        "\tzipcodes = df[\"zipcode\"].value_counts().keys().tolist()\n",
        "\tcounts = df[\"zipcode\"].value_counts().tolist()\n",
        "\n",
        "\t# loop over each of the unique zip codes and their corresponding\n",
        "\t# count\n",
        "\tfor (zipcode, count) in zip(zipcodes, counts):\n",
        "\t\t# the zip code counts for our housing dataset is *extremely*\n",
        "\t\t# unbalanced (some only having 1 or 2 houses per zip code)\n",
        "\t\t# so let's sanitize our data by removing any houses with less\n",
        "\t\t# than 25 houses per zip code\n",
        "\t\tif count < 25:\n",
        "\t\t\tidxs = df[df[\"zipcode\"] == zipcode].index\n",
        "\t\t\tdf.drop(idxs, inplace=True)\n",
        "\n",
        "\t# return the data frame\n",
        "\treturn df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCR-9Vpx2LPE"
      },
      "source": [
        "def load_house_images(df, inputPath):\n",
        "\t# initialize our images array (i.e., the house images themselves)\n",
        "\timages = []\n",
        "\n",
        "\t# loop over the indexes of the houses\n",
        "\tfor i in df.index.values:\n",
        "\t\t# find the four images for the house and sort the file paths,\n",
        "\t\t# ensuring the four are always in the *same order*\n",
        "\t\tbasePath = os.path.sep.join([inputPath, \"{}_*\".format(i + 1)])\n",
        "\t\thousePaths = sorted(list(glob.glob(basePath)))\n",
        "\n",
        "\t\t# initialize our list of input images along with the output image\n",
        "\t\t# after *combining* the four input images\n",
        "\t\tinputImages = []\n",
        "\t\toutputImage = np.zeros((64, 64, 3), dtype=\"uint8\")\n",
        "\n",
        "\t\t# loop over the input house paths\n",
        "\t\tfor housePath in housePaths:\n",
        "\t\t\t# load the input image, resize it to be 32 32, and then\n",
        "\t\t\t# update the list of input images\n",
        "\t\t\timage = cv2.imread(housePath)\n",
        "\t\t\timage = cv2.resize(image, (32, 32))\n",
        "\t\t\tinputImages.append(image)\n",
        "\n",
        "\t\t# tile the four input images in the output image such the first\n",
        "\t\t# image goes in the top-right corner, the second image in the\n",
        "\t\t# top-left corner, the third image in the bottom-right corner,\n",
        "\t\t# and the final image in the bottom-left corner\n",
        "\t\toutputImage[0:32, 0:32] = inputImages[0]\n",
        "\t\toutputImage[0:32, 32:64] = inputImages[1]\n",
        "\t\toutputImage[32:64, 32:64] = inputImages[2]\n",
        "\t\toutputImage[32:64, 0:32] = inputImages[3]\n",
        "\n",
        "\t\t# add the tiled image to our set of images the network will be\n",
        "\t\t# trained on\n",
        "\t\timages.append(outputImage)\n",
        "\n",
        "\t# return our set of images\n",
        "\treturn np.array(images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edKTOmrj2QhR"
      },
      "source": [
        "### Using Keras to implement a CNN for regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mj1uxU8l2Ma_"
      },
      "source": [
        "def create_cnn(width, height, depth, filters=(16, 32, 64), regress=False):\n",
        "\t# initialize the input shape and channel dimension, assuming\n",
        "\t# TensorFlow/channels-last ordering\n",
        "\tinputShape = (height, width, depth)\n",
        "\tchanDim = -1\n",
        "\n",
        "\t# define the model input\n",
        "\tinputs = Input(shape=inputShape)\n",
        "\n",
        "\t# loop over the number of filters\n",
        "\tfor (i, f) in enumerate(filters):\n",
        "\t\t# if this is the first CONV layer then set the input\n",
        "\t\t# appropriately\n",
        "\t\tif i == 0:\n",
        "\t\t\tx = inputs\n",
        "\n",
        "\t\t# CONV => RELU => BN => POOL\n",
        "\t\tx = Conv2D(f, (3, 3), padding=\"same\")(x)\n",
        "\t\tx = Activation(\"relu\")(x)\n",
        "\t\tx = BatchNormalization(axis=chanDim)(x)\n",
        "\t\tx = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "\t# flatten the volume, then FC => RELU => BN => DROPOUT\n",
        "\tx = Flatten()(x)\n",
        "\tx = Dense(16)(x)\n",
        "\tx = Activation(\"relu\")(x)\n",
        "\tx = BatchNormalization(axis=chanDim)(x)\n",
        "\tx = Dropout(0.5)(x)\n",
        "\n",
        "\t# apply another FC layer, this one to match the number of nodes\n",
        "\t# coming out of the MLP\n",
        "\tx = Dense(4)(x)\n",
        "\tx = Activation(\"relu\")(x)\n",
        "\n",
        "\t# check to see if the regression node should be added\n",
        "\tif regress:\n",
        "\t\tx = Dense(1, activation=\"linear\")(x)\n",
        "\n",
        "\t# construct the CNN\n",
        "\tmodel = Model(inputs, x)\n",
        "\n",
        "\t# return the CNN\n",
        "\treturn model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xL0Mb5IS2eZl"
      },
      "source": [
        "### Implementing the regression training script"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N56JsdX62bJt"
      },
      "source": [
        "# # construct the argument parser and parse the arguments\n",
        "# ap = argparse.ArgumentParser()\n",
        "# ap.add_argument(\"-d\", \"--dataset\", type=str, required=True,\n",
        "# \thelp=\"path to input dataset of house images\")\n",
        "# args = vars(ap.parse_args())\n",
        "\n",
        "# since we are using Jupyter Notebooks we can replace our argument\n",
        "# parsing code with *hard coded* arguments and values\n",
        "args = {\n",
        "\t\"dataset\": \"Houses-dataset/Houses Dataset/\"\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKDdbz9_26Y9"
      },
      "source": [
        "# construct the path to the input .txt file that contains information\n",
        "# on each house in the dataset and then load the dataset\n",
        "print(\"[INFO] loading house attributes...\")\n",
        "inputPath = os.path.sep.join([args[\"dataset\"], \"HousesInfo.txt\"])\n",
        "df = load_house_attributes(inputPath)\n",
        "\n",
        "# load the house images and then scale the pixel intensities to the\n",
        "# range [0, 1]\n",
        "print(\"[INFO] loading house images...\")\n",
        "images = load_house_images(df, args[\"dataset\"])\n",
        "images = images / 255.0\n",
        "\n",
        "# partition the data into training and testing splits using 75% of\n",
        "# the data for training and the remaining 25% for testing\n",
        "split = train_test_split(df, images, test_size=0.25, random_state=42)\n",
        "(trainAttrX, testAttrX, trainImagesX, testImagesX) = split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rf1nZsX2-1h"
      },
      "source": [
        "# find the largest house price in the training set and use it to\n",
        "# scale our house prices to the range [0, 1] (will lead to better\n",
        "# training and convergence)\n",
        "maxPrice = trainAttrX[\"price\"].max()\n",
        "trainY = trainAttrX[\"price\"] / maxPrice\n",
        "testY = testAttrX[\"price\"] / maxPrice\n",
        "\n",
        "# create our Convolutional Neural Network and then compile the model\n",
        "# using mean absolute percentage error as our loss, implying that we\n",
        "# seek to minimize the absolute percentage difference between our\n",
        "# price *predictions* and the *actual prices*\n",
        "model = create_cnn(64, 64, 3, regress=True)\n",
        "opt = Adam(lr=1e-3, decay=1e-3 / 200)\n",
        "model.compile(loss=\"mean_absolute_percentage_error\", optimizer=opt)\n",
        "\n",
        "# train the model\n",
        "print(\"[INFO] training model...\")\n",
        "model.fit(x=trainImagesX, y=trainY, \n",
        "    validation_data=(testImagesX, testY),\n",
        "    epochs=200, batch_size=8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JeQWtKbo3USG"
      },
      "source": [
        "# make predictions on the testing data\n",
        "print(\"[INFO] predicting house prices...\")\n",
        "preds = model.predict(testImagesX)\n",
        "\n",
        "# compute the difference between the *predicted* house prices and the\n",
        "# *actual* house prices, then compute the percentage difference and\n",
        "# the absolute percentage difference\n",
        "diff = preds.flatten() - testY\n",
        "percentDiff = (diff / testY) * 100\n",
        "absPercentDiff = np.abs(percentDiff)\n",
        "\n",
        "# compute the mean and standard deviation of the absolute percentage\n",
        "# difference\n",
        "mean = np.mean(absPercentDiff)\n",
        "std = np.std(absPercentDiff)\n",
        "\n",
        "# finally, show some statistics on our model\n",
        "locale.setlocale(locale.LC_ALL, \"en_US.UTF-8\")\n",
        "print(\"[INFO] avg. house price: {}, std house price: {}\".format(\n",
        "\tlocale.currency(df[\"price\"].mean(), grouping=True),\n",
        "\tlocale.currency(df[\"price\"].std(), grouping=True)))\n",
        "print(\"[INFO] mean: {:.2f}%, std: {:.2f}%\".format(mean, std))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ogkNauArL6u"
      },
      "source": [
        "For a detailed walkthrough of the concepts and code, be sure to refer to the full tutorial, [*Keras, Regression, and CNNs*](https://www.pyimagesearch.com/2019/01/28/keras-regression-and-cnns/) published on 2019-01-28."
      ]
    }
  ]
}