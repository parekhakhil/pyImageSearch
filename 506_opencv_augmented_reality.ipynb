{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "506-opencv_augmented_reality.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/parekhakhil/pyImageSearch/blob/main/506_opencv_augmented_reality.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3_EI2A0ngV9"
      },
      "source": [
        "# OpenCV Augmented Reality (AR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IY4MmNc4nkPh"
      },
      "source": [
        "\n",
        "This notebook is associated with the [OpenCV Augmented Reality (AR)](https://www.pyimagesearch.com/2021/01/04/opencv-augmented-reality-ar/) blog post published on 01-04-21.\n",
        "\n",
        "The entire blog post is not recreated here; be sure to refer to the blog post to learn the concepts and read the complete code walkthrough. Only the code for the blog post is here and most codeblocks have a 1:1 relationship with what you find in the blog post with two exceptions: (1) Python classes are not separate files as they are typically organized with PyImageSearch projects and (2) Command Line Argument parsing is replaced with an `args` dictionary that you can manipulate as needed. Our recommmendation is that you run the code block-by-block as-is prior to adjusting parameters and `args` inputs. Once you've verified that the code is working, you are welcome to hack with it and learn from manipulating inputs, settings, and parameters. For more information on using Jupyter and Colab, please refer to these resources:\n",
        "\n",
        "*   [Jupyter Notebook User Interface](https://jupyter-notebook.readthedocs.io/en/stable/notebook.html#notebook-user-interface)\n",
        "*   [Overview of Google Colaboratory Features](https://colab.research.google.com/notebooks/basic_features_overview.ipynb)\n",
        "\n",
        "\n",
        "\n",
        "Happy hacking!\n",
        "\n",
        "\n",
        "<hr>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVK8fRlQp3HM"
      },
      "source": [
        "### Download the code zip file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrGqgNmKnZ5Q"
      },
      "source": [
        "!wget https://pyimagesearch-code-downloads.s3-us-west-2.amazonaws.com/opencv-augmented-reality/opencv-augmented-reality.zip\n",
        "!unzip -qq opencv-augmented-reality.zip\n",
        "%cd opencv-augmented-reality"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YECVlWoOqCj1"
      },
      "source": [
        "## Blog Post Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_vbSPmvq8zL"
      },
      "source": [
        "### Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdnfQoQrYzXk"
      },
      "source": [
        "# import the necessary packages\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import argparse\n",
        "import imutils\n",
        "import cv2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWOLob4IScbu"
      },
      "source": [
        "### Function to display images in Jupyter Notebooks and Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ye-62UuxShFQ"
      },
      "source": [
        "def plt_imshow(title, image):\n",
        "    # convert the image frame BGR to RGB color space and display it\n",
        "\timage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\tplt.imshow(image)\n",
        "\tplt.title(title)\n",
        "\tplt.grid(False)\n",
        "\tplt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFsBBx8lZCqm"
      },
      "source": [
        "### Implementing augmented reality with OpenCV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzqxtm6NY0pq"
      },
      "source": [
        "# # construct the argument parser and parse the arguments\n",
        "# ap = argparse.ArgumentParser()\n",
        "# ap.add_argument(\"-i\", \"--image\", required=True,\n",
        "# \thelp=\"path to input image containing ArUCo tag\")\n",
        "# ap.add_argument(\"-s\", \"--source\", required=True,\n",
        "# \thelp=\"path to input source image that will be put on input\")\n",
        "# args = vars(ap.parse_args())\n",
        "\n",
        "# since we are using Jupyter Notebooks we can replace our argument\n",
        "# parsing code with *hard coded* arguments and values\n",
        "args = {\n",
        "    \"image\": \"examples/input_01.jpg\",\n",
        "    \"source\": \"sources/squirrel.jpg\"\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8ISaXprZbcF"
      },
      "source": [
        "# load the input image from disk, resize it, and grab its spatial\n",
        "# dimensions\n",
        "print(\"[INFO] loading input image and source image...\")\n",
        "image = cv2.imread(args[\"image\"])\n",
        "image = imutils.resize(image, width=600)\n",
        "(imgH, imgW) = image.shape[:2]\n",
        "\n",
        "# load the source image from disk\n",
        "source = cv2.imread(args[\"source\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AG4LNyjZfy_"
      },
      "source": [
        "# load the ArUCo dictionary, grab the ArUCo parameters, and detect\n",
        "# the markers\n",
        "print(\"[INFO] detecting markers...\")\n",
        "arucoDict = cv2.aruco.Dictionary_get(cv2.aruco.DICT_ARUCO_ORIGINAL)\n",
        "arucoParams = cv2.aruco.DetectorParameters_create()\n",
        "(corners, ids, rejected) = cv2.aruco.detectMarkers(image, arucoDict,\n",
        "\tparameters=arucoParams)\n",
        "\n",
        "# if we have not found four markers in the input image then we cannot\n",
        "# apply our augmented reality technique\n",
        "if len(corners) != 4:\n",
        "\tprint(\"[INFO] could not find 4 corners...exiting\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEbcOGWEZoLm"
      },
      "source": [
        "# otherwise, we've found the four ArUco markers, so we can continue\n",
        "# by flattening the ArUco IDs list and initializing our list of\n",
        "# reference points\n",
        "print(\"[INFO] constructing augmented reality visualization...\")\n",
        "ids = ids.flatten()\n",
        "refPts = []\n",
        "\n",
        "# loop over the IDs of the ArUco markers in top-left, top-right,\n",
        "# bottom-right, and bottom-left order\n",
        "for i in (923, 1001, 241, 1007):\n",
        "\t# grab the index of the corner with the current ID and append the\n",
        "\t# corner (x, y)-coordinates to our list of reference points\n",
        "\tj = np.squeeze(np.where(ids == i))\n",
        "\tcorner = np.squeeze(corners[j])\n",
        "\trefPts.append(corner)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1l9xrECGZxUu"
      },
      "source": [
        "# unpack our ArUco reference points and use the reference points to\n",
        "# define the *destination* transform matrix, making sure the points\n",
        "# are specified in top-left, top-right, bottom-right, and bottom-left\n",
        "# order\n",
        "(refPtTL, refPtTR, refPtBR, refPtBL) = refPts\n",
        "dstMat = [refPtTL[0], refPtTR[1], refPtBR[2], refPtBL[3]]\n",
        "dstMat = np.array(dstMat)\n",
        "\n",
        "# grab the spatial dimensions of the source image and define the\n",
        "# transform matrix for the *source* image in top-left, top-right,\n",
        "# bottom-right, and bottom-left order\n",
        "(srcH, srcW) = source.shape[:2]\n",
        "srcMat = np.array([[0, 0], [srcW, 0], [srcW, srcH], [0, srcH]])\n",
        "\n",
        "# compute the homography matrix and then warp the source image to the\n",
        "# destination based on the homography\n",
        "(H, _) = cv2.findHomography(srcMat, dstMat)\n",
        "warped = cv2.warpPerspective(source, H, (imgW, imgH))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kobQlvimZ5Ph"
      },
      "source": [
        "# construct a mask for the source image now that the perspective warp\n",
        "# has taken place (we'll need this mask to copy the source image into\n",
        "# the destination)\n",
        "mask = np.zeros((imgH, imgW), dtype=\"uint8\")\n",
        "cv2.fillConvexPoly(mask, dstMat.astype(\"int32\"), (255, 255, 255),\n",
        "\tcv2.LINE_AA)\n",
        "\n",
        "# this step is optional, but to give the source image a black border\n",
        "# surrounding it when applied to the source image, you can apply a\n",
        "# dilation operation\n",
        "rect = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))\n",
        "mask = cv2.dilate(mask, rect, iterations=2)\n",
        "\n",
        "# create a three channel version of the mask by stacking it depth-wise,\n",
        "# such that we can copy the warped source image into the input image\n",
        "maskScaled = mask.copy() / 255.0\n",
        "maskScaled = np.dstack([maskScaled] * 3)\n",
        "\n",
        "# copy the warped source image into the input image by (1) multiplying\n",
        "# the warped image and masked together, (2) multiplying the original\n",
        "# input image with the mask (giving more weight to the input where\n",
        "# there *ARE NOT* masked pixels), and (3) adding the resulting\n",
        "# multiplications together\n",
        "warpedMultiplied = cv2.multiply(warped.astype(\"float\"), maskScaled)\n",
        "imageMultiplied = cv2.multiply(image.astype(float), 1.0 - maskScaled)\n",
        "output = cv2.add(warpedMultiplied, imageMultiplied)\n",
        "output = output.astype(\"uint8\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0OmxNoLaDix"
      },
      "source": [
        "# show the input image, source image, output of our augmented reality\n",
        "plt_imshow(\"Input\", image)\n",
        "plt_imshow(\"Source\", source)\n",
        "plt_imshow(\"OpenCV AR Output\", output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PO72wi-UyCIH"
      },
      "source": [
        "For a detailed walkthrough of the concepts and code, be sure to refer to the full tutorial, [*OpenCV Augmented Reality (AR)*](https://www.pyimagesearch.com/2021/01/04/opencv-augmented-reality-ar/) blog post published on 01-04-21."
      ]
    }
  ]
}