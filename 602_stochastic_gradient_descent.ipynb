{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "602-stochastic_gradient_descent.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/parekhakhil/pyImageSearch/blob/main/602_stochastic_gradient_descent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4RpK9pawQzP"
      },
      "source": [
        "# Stochastic Gradient Descent (SGD) with Python\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ntZ1AkXZIxY"
      },
      "source": [
        "\n",
        "This notebook is associated with the [Stochastic Gradient Descent (SGD) with Python](https://www.pyimagesearch.com/2016/10/17/stochastic-gradient-descent-sgd-with-python/) blog post published on 2016-10-17.\n",
        "\n",
        "Only the code for the blog post is here. Most codeblocks have a 1:1 relationship with what you find in the blog post with two exceptions: (1) Python classes are not separate files as they are typically organized with PyImageSearch projects, and (2) Command Line Argument parsing is replaced with an `args` dictionary that you can manipulate as needed.\n",
        "\n",
        "We recommend that you execute (press ▶️) the code block-by-block, as-is, before adjusting parameters and `args` inputs. Once you've verified that the code is working, you are welcome to hack with it and learn from manipulating inputs, settings, and parameters. For more information on using Jupyter and Colab, please refer to these resources:\n",
        "\n",
        "*   [Jupyter Notebook User Interface](https://jupyter-notebook.readthedocs.io/en/stable/notebook.html#notebook-user-interface)\n",
        "*   [Overview of Google Colaboratory Features](https://colab.research.google.com/notebooks/basic_features_overview.ipynb)\n",
        "\n",
        " \n",
        "\n",
        "Happy hacking!\n",
        "\n",
        "\n",
        "\n",
        "<hr>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFhAzQB3aNMa"
      },
      "source": [
        "### Download the code zip file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7y0LG1EuaRlB"
      },
      "source": [
        "!wget https://www.pyimagesearch.com/wp-content/uploads/2016/08/stochastic-gradient-descent.zip\n",
        "!unzip -qq stochastic-gradient-descent.zip\n",
        "%cd stochastic-gradient-descent"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SgTVT3HagGZ"
      },
      "source": [
        "## Blog Post Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcrOk6pURp50"
      },
      "source": [
        "### Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJaCNlDDRz6d"
      },
      "source": [
        "# import the necessary packages\n",
        "from sklearn.datasets.samples_generator import make_blobs\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import argparse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTk6EY43-5We"
      },
      "source": [
        "### Implementing Stochastic Gradient Descent (SGD) with Python"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkybQERh-lLI"
      },
      "source": [
        "def sigmoid_activation(x):\n",
        "\t# compute and return the sigmoid activation value for a\n",
        "\t# given input value\n",
        "\treturn 1.0 / (1 + np.exp(-x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3mcSCMv_Mlm"
      },
      "source": [
        "def next_batch(X, y, batchSize):\n",
        "\t# loop over our dataset `X` in mini-batches of size `batchSize`\n",
        "\tfor i in np.arange(0, X.shape[0], batchSize):\n",
        "\t\t# yield a tuple of the current batched data and labels\n",
        "\t\tyield (X[i:i + batchSize], y[i:i + batchSize])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d61GTWbZ_M-_"
      },
      "source": [
        "# construct the argument parse and parse the arguments\n",
        "# ap = argparse.ArgumentParser()\n",
        "# ap.add_argument(\"-e\", \"--epochs\", type=float, default=100,\n",
        "# \thelp=\"# of epochs\")\n",
        "# ap.add_argument(\"-a\", \"--alpha\", type=float, default=0.01,\n",
        "# \thelp=\"learning rate\")\n",
        "# ap.add_argument(\"-b\", \"--batch-size\", type=int, default=32,\n",
        "# \thelp=\"size of SGD mini-batches\")\n",
        "# args = vars(ap.parse_args())\n",
        "\n",
        "# since we are using Jupyter Notebooks we can replace our argument\n",
        "# parsing code with *hard coded* arguments and values\n",
        "args = {\n",
        "\t\"epochs\": 100,\n",
        "\t\"alpha\": 0.01,\n",
        "    \"batch_size\": 32\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2DSnY5O_ayf"
      },
      "source": [
        "# generate a 2-class classification problem with 400 data points,\n",
        "# where each data point is a 2D feature vector\n",
        "(X, y) = make_blobs(n_samples=400, n_features=2, centers=2,\n",
        "\tcluster_std=2.5, random_state=95)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thuvQNW3_eo3"
      },
      "source": [
        "# insert a column of 1's as the first entry in the feature\n",
        "# vector -- this is a little trick that allows us to treat\n",
        "# the bias as a trainable parameter *within* the weight matrix\n",
        "# rather than an entirely separate variable\n",
        "X = np.c_[np.ones((X.shape[0])), X]\n",
        "\n",
        "# initialize our weight matrix such it has the same number of\n",
        "# columns as our input features\n",
        "print(\"[INFO] starting training...\")\n",
        "W = np.random.uniform(size=(X.shape[1],))\n",
        "\n",
        "# initialize a list to store the loss value for each epoch\n",
        "lossHistory = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4FloDKY_jZY"
      },
      "source": [
        "# loop over the desired number of epochs\n",
        "for epoch in np.arange(0, args[\"epochs\"]):\n",
        "\t# initialize the total loss for the epoch\n",
        "\tepochLoss = []\n",
        "\n",
        "\t# loop over our data in batches\n",
        "\tfor (batchX, batchY) in next_batch(X, y, args[\"batch_size\"]):\n",
        "\t\t# take the dot product between our current batch of\n",
        "\t\t# features and weight matrix `W`, then pass this value\n",
        "\t\t# through the sigmoid activation function\n",
        "\t\tpreds = sigmoid_activation(batchX.dot(W))\n",
        "\n",
        "\t\t# now that we have our predictions, we need to determine\n",
        "\t\t# our `error`, which is the difference between our predictions\n",
        "\t\t# and the true values\n",
        "\t\terror = preds - batchY\n",
        "\n",
        "\t\t# given our `error`, we can compute the total loss value on\n",
        "\t\t# the batch as the sum of squared loss\n",
        "\t\tloss = np.sum(error ** 2)\n",
        "\t\tepochLoss.append(loss)\n",
        "\n",
        "\t\t# the gradient update is therefore the dot product between\n",
        "\t\t# the transpose of our current batch and the error on the\n",
        "\t\t# # batch\n",
        "\t\tgradient = batchX.T.dot(error) / batchX.shape[0]\n",
        "\n",
        "\t\t# use the gradient computed on the current batch to take\n",
        "\t\t# a \"step\" in the correct direction\n",
        "\t\tW += -args[\"alpha\"] * gradient\n",
        "\n",
        "\t# update our loss history list by taking the average loss\n",
        "\t# across all batches\n",
        "\tlossHistory.append(np.average(epochLoss))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBNb52ao_pSA"
      },
      "source": [
        "# compute the line of best fit by setting the sigmoid function\n",
        "# to 0 and solving for X2 in terms of X1\n",
        "Y = (-W[0] - (W[1] * X)) / W[2]\n",
        "\n",
        "# plot the original data along with our line of best fit\n",
        "plt.figure()\n",
        "plt.scatter(X[:, 1], X[:, 2], marker=\"o\", c=y)\n",
        "plt.plot(X, Y, \"r-\")\n",
        "\n",
        "# construct a figure that plots the loss over time\n",
        "fig = plt.figure()\n",
        "plt.plot(np.arange(0, args[\"epochs\"]), lossHistory)\n",
        "fig.suptitle(\"Training Loss\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ogkNauArL6u"
      },
      "source": [
        "For a detailed walkthrough of the concepts and code, be sure to refer to the full tutorial, [*Stochastic Gradient Descent (SGD) with Python*](https://www.pyimagesearch.com/2016/10/17/stochastic-gradient-descent-sgd-with-python/) blog post published on 2016-10-17."
      ]
    }
  ]
}