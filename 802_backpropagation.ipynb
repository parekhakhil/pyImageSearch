{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "802-backpropagation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/parekhakhil/pyImageSearch/blob/main/802_backpropagation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4RpK9pawQzP"
      },
      "source": [
        "# Backpropagation from scratch with Python\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ntZ1AkXZIxY"
      },
      "source": [
        "\n",
        "This notebook is associated with the [Backpropagation from scratch with Python](pyimagesearch.com/2021/05/06/backpropagation-from-scratch-with-python/) blog post published on 2021-05-06.\n",
        "\n",
        "Only the code for the blog post is here. Most codeblocks have a 1:1 relationship with what you find in the blog post with two exceptions: (1) Python classes are not separate files as they are typically organized with PyImageSearch projects, and (2) Command Line Argument parsing is replaced with an `args` dictionary that you can manipulate as needed.\n",
        "\n",
        "We recommend that you execute (press ▶️) the code block-by-block, as-is, before adjusting parameters and `args` inputs. Once you've verified that the code is working, you are welcome to hack with it and learn from manipulating inputs, settings, and parameters. For more information on using Jupyter and Colab, please refer to these resources:\n",
        "\n",
        "*   [Jupyter Notebook User Interface](https://jupyter-notebook.readthedocs.io/en/stable/notebook.html#notebook-user-interface)\n",
        "*   [Overview of Google Colaboratory Features](https://colab.research.google.com/notebooks/basic_features_overview.ipynb)\n",
        "\n",
        "\n",
        "Happy hacking!\n",
        "\n",
        "\n",
        "\n",
        "<hr>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFhAzQB3aNMa"
      },
      "source": [
        "### Download the code zip file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7y0LG1EuaRlB"
      },
      "source": [
        "!wget https://pyimagesearch-code-downloads.s3-us-west-2.amazonaws.com/backpropagation-python/backpropagation-python.zip\n",
        "!unzip -qq backpropagation-python.zip\n",
        "%cd backpropagation-python"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SgTVT3HagGZ"
      },
      "source": [
        "## Blog Post Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcrOk6pURp50"
      },
      "source": [
        "### Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJaCNlDDRz6d"
      },
      "source": [
        "# import the necessary packages\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn import datasets\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-wl7uQITgQs"
      },
      "source": [
        "### Implementing Backpropagation with Python"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewNgZoRvTcgV"
      },
      "source": [
        "class NeuralNetwork:\n",
        "\tdef __init__(self, layers, alpha=0.1):\n",
        "\t\t# initialize the list of weights matrices, then store the\n",
        "\t\t# network architecture and learning rate\n",
        "\t\tself.W = []\n",
        "\t\tself.layers = layers\n",
        "\t\tself.alpha = alpha\n",
        "\n",
        "\t\t# start looping from the index of the first layer but\n",
        "\t\t# stop before we reach the last two layers\n",
        "\t\tfor i in np.arange(0, len(layers) - 2):\n",
        "\t\t\t# randomly initialize a weight matrix connecting the\n",
        "\t\t\t# number of nodes in each respective layer together,\n",
        "\t\t\t# adding an extra node for the bias\n",
        "\t\t\tw = np.random.randn(layers[i] + 1, layers[i + 1] + 1)\n",
        "\t\t\tself.W.append(w / np.sqrt(layers[i]))\n",
        "\n",
        "\t\t# the last two layers are a special case where the input\n",
        "\t\t# connections need a bias term but the output does not\n",
        "\t\tw = np.random.randn(layers[-2] + 1, layers[-1])\n",
        "\t\tself.W.append(w / np.sqrt(layers[-2]))\n",
        "\n",
        "\tdef __repr__(self):\n",
        "\t\t# construct and return a string that represents the network\n",
        "\t\t# architecture\n",
        "\t\treturn \"NeuralNetwork: {}\".format(\n",
        "\t\t\t\"-\".join(str(l) for l in self.layers))\n",
        "\n",
        "\tdef sigmoid(self, x):\n",
        "\t\t# compute and return the sigmoid activation value for a\n",
        "\t\t# given input value\n",
        "\t\treturn 1.0 / (1 + np.exp(-x))\n",
        "\n",
        "\tdef sigmoid_deriv(self, x):\n",
        "\t\t# compute the derivative of the sigmoid function ASSUMING\n",
        "\t\t# that `x` has already been passed through the `sigmoid`\n",
        "\t\t# function\n",
        "\t\treturn x * (1 - x)\n",
        "\n",
        "\tdef fit(self, X, y, epochs=1000, displayUpdate=100):\n",
        "\t\t# insert a column of 1's as the last entry in the feature\n",
        "\t\t# matrix -- this little trick allows us to treat the bias\n",
        "\t\t# as a trainable parameter within the weight matrix\n",
        "\t\tX = np.c_[X, np.ones((X.shape[0]))]\n",
        "\n",
        "\t\t# loop over the desired number of epochs\n",
        "\t\tfor epoch in np.arange(0, epochs):\n",
        "\t\t\t# loop over each individual data point and train\n",
        "\t\t\t# our network on it\n",
        "\t\t\tfor (x, target) in zip(X, y):\n",
        "\t\t\t\tself.fit_partial(x, target)\n",
        "\n",
        "\t\t\t# check to see if we should display a training update\n",
        "\t\t\tif epoch == 0 or (epoch + 1) % displayUpdate == 0:\n",
        "\t\t\t\tloss = self.calculate_loss(X, y)\n",
        "\t\t\t\tprint(\"[INFO] epoch={}, loss={:.7f}\".format(\n",
        "\t\t\t\t\tepoch + 1, loss))\n",
        "\n",
        "\tdef fit_partial(self, x, y):\n",
        "\t\t# construct our list of output activations for each layer\n",
        "\t\t# as our data point flows through the network; the first\n",
        "\t\t# activation is a special case -- it's just the input\n",
        "\t\t# feature vector itself\n",
        "\t\tA = [np.atleast_2d(x)]\n",
        "\n",
        "\t\t# FEEDFORWARD:\n",
        "\t\t# loop over the layers in the network\n",
        "\t\tfor layer in np.arange(0, len(self.W)):\n",
        "\t\t\t# feedforward the activation at the current layer by\n",
        "\t\t\t# taking the dot product between the activation and\n",
        "\t\t\t# the weight matrix -- this is called the \"net input\"\n",
        "\t\t\t# to the current layer\n",
        "\t\t\tnet = A[layer].dot(self.W[layer])\n",
        "\n",
        "\t\t\t# computing the \"net output\" is simply applying our\n",
        "\t\t\t# non-linear activation function to the net input\n",
        "\t\t\tout = self.sigmoid(net)\n",
        "\n",
        "\t\t\t# once we have the net output, add it to our list of\n",
        "\t\t\t# activations\n",
        "\t\t\tA.append(out)\n",
        "\n",
        "\t\t# BACKPROPAGATION\n",
        "\t\t# the first phase of backpropagation is to compute the\n",
        "\t\t# difference between our *prediction* (the final output\n",
        "\t\t# activation in the activations list) and the true target\n",
        "\t\t# value\n",
        "\t\terror = A[-1] - y\n",
        "\n",
        "\t\t# from here, we need to apply the chain rule and build our\n",
        "\t\t# list of deltas `D`; the first entry in the deltas is\n",
        "\t\t# simply the error of the output layer times the derivative\n",
        "\t\t# of our activation function for the output value\n",
        "\t\tD = [error * self.sigmoid_deriv(A[-1])]\n",
        "\n",
        "\t\t# once you understand the chain rule it becomes super easy\n",
        "\t\t# to implement with a `for` loop -- simply loop over the\n",
        "\t\t# layers in reverse order (ignoring the last two since we\n",
        "\t\t# already have taken them into account)\n",
        "\t\tfor layer in np.arange(len(A) - 2, 0, -1):\n",
        "\t\t\t# the delta for the current layer is equal to the delta\n",
        "\t\t\t# of the *previous layer* dotted with the weight matrix\n",
        "\t\t\t# of the current layer, followed by multiplying the delta\n",
        "\t\t\t# by the derivative of the non-linear activation function\n",
        "\t\t\t# for the activations of the current layer\n",
        "\t\t\tdelta = D[-1].dot(self.W[layer].T)\n",
        "\t\t\tdelta = delta * self.sigmoid_deriv(A[layer])\n",
        "\t\t\tD.append(delta)\n",
        "\n",
        "\t\t# since we looped over our layers in reverse order we need to\n",
        "\t\t# reverse the deltas\n",
        "\t\tD = D[::-1]\n",
        "\n",
        "\t\t# WEIGHT UPDATE PHASE\n",
        "\t\t# loop over the layers\n",
        "\t\tfor layer in np.arange(0, len(self.W)):\n",
        "\t\t\t# update our weights by taking the dot product of the layer\n",
        "\t\t\t# activations with their respective deltas, then multiplying\n",
        "\t\t\t# this value by some small learning rate and adding to our\n",
        "\t\t\t# weight matrix -- this is where the actual \"learning\" takes\n",
        "\t\t\t# place\n",
        "\t\t\tself.W[layer] += -self.alpha * A[layer].T.dot(D[layer])\n",
        "\n",
        "\tdef predict(self, X, addBias=True):\n",
        "\t\t# initialize the output prediction as the input features -- this\n",
        "\t\t# value will be (forward) propagated through the network to\n",
        "\t\t# obtain the final prediction\n",
        "\t\tp = np.atleast_2d(X)\n",
        "\n",
        "\t\t# check to see if the bias column should be added\n",
        "\t\tif addBias:\n",
        "\t\t\t# insert a column of 1's as the last entry in the feature\n",
        "\t\t\t# matrix (bias)\n",
        "\t\t\tp = np.c_[p, np.ones((p.shape[0]))]\n",
        "\n",
        "\t\t# loop over our layers in the network\n",
        "\t\tfor layer in np.arange(0, len(self.W)):\n",
        "\t\t\t# computing the output prediction is as simple as taking\n",
        "\t\t\t# the dot product between the current activation value `p`\n",
        "\t\t\t# and the weight matrix associated with the current layer,\n",
        "\t\t\t# then passing this value through a non-linear activation\n",
        "\t\t\t# function\n",
        "\t\t\tp = self.sigmoid(np.dot(p, self.W[layer]))\n",
        "\n",
        "\t\t# return the predicted value\n",
        "\t\treturn p\n",
        "\n",
        "\tdef calculate_loss(self, X, targets):\n",
        "\t\t# make predictions for the input data points then compute\n",
        "\t\t# the loss\n",
        "\t\ttargets = np.atleast_2d(targets)\n",
        "\t\tpredictions = self.predict(X, addBias=False)\n",
        "\t\tloss = 0.5 * np.sum((predictions - targets) ** 2)\n",
        "\n",
        "\t\t# return the loss\n",
        "\t\treturn loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYIbFsn_TuLC"
      },
      "source": [
        "### Backpropagation with Python Example #1: Bitwise XOR\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPDMqmHdTox1"
      },
      "source": [
        "# construct the XOR dataset\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y = np.array([[0], [1], [1], [0]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hABLm9DAT0U8"
      },
      "source": [
        "# define our 2-2-1 neural network and train it\n",
        "nn = NeuralNetwork([2, 2, 1], alpha=0.5)\n",
        "nn.fit(X, y, epochs=20000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gum7JTQsT3M1"
      },
      "source": [
        "# now that our network is trained, loop over the XOR data points\n",
        "for (x, target) in zip(X, y):\n",
        "\t# make a prediction on the data point and display the result\n",
        "\t# to our console\n",
        "\tpred = nn.predict(x)[0][0]\n",
        "\tstep = 1 if pred > 0.5 else 0\n",
        "\tprint(\"[INFO] data={}, ground-truth={}, pred={:.4f}, step={}\".format(\n",
        "\t\tx, target[0], pred, step))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkzHJmBXT5cr"
      },
      "source": [
        "# define our 2-1 neural network and train it\n",
        "nn = NeuralNetwork([2, 1], alpha=0.5)\n",
        "nn.fit(X, y, epochs=20000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoNmn3aeUAZM"
      },
      "source": [
        "# now that our network is trained, loop over the XOR data points\n",
        "for (x, target) in zip(X, y):\n",
        "\t# make a prediction on the data point and display the result\n",
        "\t# to our console\n",
        "\tpred = nn.predict(x)[0][0]\n",
        "\tstep = 1 if pred > 0.5 else 0\n",
        "\tprint(\"[INFO] data={}, ground-truth={}, pred={:.4f}, step={}\".format(\n",
        "\t\tx, target[0], pred, step))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hULE5GK0UQCQ"
      },
      "source": [
        "### Backpropagation with Python Example: MNIST Sample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbvJxGM1UStt"
      },
      "source": [
        "# load the MNIST dataset and apply min/max scaling to scale the\n",
        "# pixel intensity values to the range [0, 1] (each image is\n",
        "# represented by an 8 x 8 = 64-dim feature vector)\n",
        "print(\"[INFO] loading MNIST (sample) dataset...\")\n",
        "digits = datasets.load_digits()\n",
        "data = digits.data.astype(\"float\")\n",
        "data = (data - data.min()) / (data.max() - data.min())\n",
        "print(\"[INFO] samples: {}, dim: {}\".format(data.shape[0],\n",
        "\tdata.shape[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmQ1WcdLUTNc"
      },
      "source": [
        "# construct the training and testing splits\n",
        "(trainX, testX, trainY, testY) = train_test_split(data,\n",
        "\tdigits.target, test_size=0.25)\n",
        "\n",
        "# convert the labels from integers to vectors\n",
        "trainY = LabelBinarizer().fit_transform(trainY)\n",
        "testY = LabelBinarizer().fit_transform(testY)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXQXjzmFUWGj"
      },
      "source": [
        "# train the network\n",
        "print(\"[INFO] training network...\")\n",
        "nn = NeuralNetwork([trainX.shape[1], 32, 16, 10])\n",
        "print(\"[INFO] {}\".format(nn))\n",
        "nn.fit(trainX, trainY, epochs=1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AkqO9bvUYYj"
      },
      "source": [
        "# evaluate the network\n",
        "print(\"[INFO] evaluating network...\")\n",
        "predictions = nn.predict(testX)\n",
        "predictions = predictions.argmax(axis=1)\n",
        "print(classification_report(testY.argmax(axis=1), predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ogkNauArL6u"
      },
      "source": [
        "For a detailed walkthrough of the concepts and code, be sure to refer to the full tutorial, [Backpropagation from scratch with Python](pyimagesearch.com/2021/05/06/backpropagation-from-scratch-with-python/) published on 2021-05-06."
      ]
    }
  ]
}